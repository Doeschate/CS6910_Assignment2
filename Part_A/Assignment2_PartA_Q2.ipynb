{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2_PartA_Q2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##ASSIGNMENT-2"
      ],
      "metadata": {
        "id": "i9oMWDx10UUx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learn how to use CNNs: train from scratch, finetune a pretrained model, use a pre-trained model as it is.\n"
      ],
      "metadata": {
        "id": "CQnccrq2ewXw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installs**"
      ],
      "metadata": {
        "id": "uifpKNB1odVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U albumentations\n",
        "!pip install \"opencv-python-headless<4.3\" #for import albumentations as A\n",
        "!pip install wandb #To install wandb and evaluate models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAGlgBb8okJh",
        "outputId": "791f2ce5-62d4-42dd-f29a-eec043a5479d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.7/dist-packages (0.1.12)\n",
            "Collecting albumentations\n",
            "  Downloading albumentations-1.1.0-py3-none-any.whl (102 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▏                            | 10 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 20 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 30 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 40 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 51 kB 28.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 61 kB 31.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 71 kB 30.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 81 kB 32.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 92 kB 34.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102 kB 37.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102 kB 37.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.4.1)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (0.18.3)\n",
            "Collecting opencv-python-headless>=4.1.1\n",
            "  Downloading opencv_python_headless-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.8 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting qudida>=0.0.4\n",
            "  Downloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations) (3.13)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations) (1.0.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (7.1.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (1.3.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2021.11.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.6.3)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (3.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.1.0)\n",
            "Installing collected packages: opencv-python-headless, qudida, albumentations\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-1.1.0 opencv-python-headless-4.5.5.64 qudida-0.0.4\n",
            "Collecting opencv-python-headless<4.3\n",
            "  Downloading opencv_python_headless-4.2.0.34-cp37-cp37m-manylinux1_x86_64.whl (21.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.6 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python-headless<4.3) (1.21.5)\n",
            "Installing collected packages: opencv-python-headless\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.5.5.64\n",
            "    Uninstalling opencv-python-headless-4.5.5.64:\n",
            "      Successfully uninstalled opencv-python-headless-4.5.5.64\n",
            "Successfully installed opencv-python-headless-4.2.0.34\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.12.11-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 30.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 71.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.8-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 56.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.2.2-cp37-cp37m-manylinux1_x86_64.whl (36 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=9ac76766a4a96d2f009bd9a65ad1205e5254a3a8390719a56bec921fe070e9ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, yaspin, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.8 setproctitle-1.2.2 shortuuid-1.0.8 smmap-5.0.0 wandb-0.12.11 yaspin-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imports**"
      ],
      "metadata": {
        "id": "Vfgl_cijfI9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2\n",
        "import glob\n",
        "import numpy as np\n",
        "import random\n",
        "import wandb\n",
        "import gc\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import chain\n",
        "enable_GPU = 0"
      ],
      "metadata": {
        "id": "qpDHokXY8ffl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enabling GPU**"
      ],
      "metadata": {
        "id": "cBA2e2NrmwYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(torch.cuda.get_device_name(0))\n",
        "enable_GPU = 1"
      ],
      "metadata": {
        "id": "UajdMgmgn3MR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download iNaturalist-12K dataset**"
      ],
      "metadata": {
        "id": "3i4cMnGWFdrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "actual_data_path = \"/content/drive/MyDrive/inaturalist_12K\""
      ],
      "metadata": {
        "id": "HI4pSHzRGWW5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34f01757-6618-4086-b34b-b99f38a6bb80"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Creating**"
      ],
      "metadata": {
        "id": "DgxaBE_29k3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get all the paths from train_data_path and returns image paths for train and validation set\n",
        "def CreateTrainDataset(actual_data_path):\n",
        "  train_data_path = os.path.join(actual_data_path, \"train\")\n",
        "  train_image_paths = [] #to store image paths in list\n",
        "  classes = [] #to store class values\n",
        "  for data_path in glob.glob(train_data_path + \"/*\"):\n",
        "    classes.append(data_path.split('/')[-1]) #stores the classes in this list\n",
        "    train_image_paths.append(glob.glob(data_path + '/*')) #stores all the training image paths in this list\n",
        "  train_image_paths = list(chain.from_iterable(train_image_paths))\n",
        "  random.shuffle(train_image_paths)\n",
        "\n",
        "  # split train valid from train paths (90,10)\n",
        "  train_image_paths, valid_image_paths = train_image_paths[:int(0.9*len(train_image_paths))], train_image_paths[int(0.9*len(train_image_paths)):] \n",
        "  return train_image_paths, valid_image_paths\n",
        "\n",
        "# create the test_image_paths\n",
        "def CreateTestDataset(actual_data_path):\n",
        "  test_data_path = os.path.join(actual_data_path, \"val\")\n",
        "  test_image_paths = []\n",
        "  for data_path in glob.glob(test_data_path + '/*'):\n",
        "      test_image_paths.append(glob.glob(data_path + '/*')) #stores all the test images path in this list\n",
        "  test_image_paths = list(chain.from_iterable(test_image_paths))\n",
        "  return test_image_paths"
      ],
      "metadata": {
        "id": "ora_zBPn7o1i"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create dictionary for class indexes\n",
        "train_data_path = os.path.join(actual_data_path, \"train\")\n",
        "classes = [] #to store class values\n",
        "for data_path in glob.glob(train_data_path + \"/*\"):\n",
        "  classes.append(data_path.split('/')[-1])\n",
        "idx_to_class = {i:j for i, j in enumerate(classes)} #index to class map\n",
        "class_to_idx = {value:key for key,value in idx_to_class.items()} #class to index map"
      ],
      "metadata": {
        "id": "8CxrpoqISjgI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Class**"
      ],
      "metadata": {
        "id": "-B7uy3GTY1gi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Function returns images and corresponding lebels after performing transforms\n",
        "class iNaturalist_12KDataset(Dataset):\n",
        "    def __init__(self, image_paths, transform=False):\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_filepath = self.image_paths[idx]\n",
        "        image = cv2.imread(image_filepath)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        label = image_filepath.split('/')[-2]\n",
        "        label = class_to_idx[label]\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image=image)[\"image\"]\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "gZs1aTpeVCst"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building the Model**"
      ],
      "metadata": {
        "id": "y53VI2iPpO7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Optimization Function\n",
        "def OptimizerFunction(model, learning_rate,weight_decay, optimizer_name):  \n",
        "  print(model, learning_rate,weight_decay, optimizer_name)\n",
        "  if optimizer_name == \"SGD\":\n",
        "    return torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "  elif optimizer_name == \"Adam\":\n",
        "    return torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
      ],
      "metadata": {
        "id": "5trpZKH-nIxp"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loss Function\n",
        "def LossFunction():\n",
        "  return nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "kb7hUIJfxQHX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'sigmoid', 'relu','elu','gelu'\n",
        "#Activation Function\n",
        "#To add another activation just add another else statement for that activation and return the corresponding pytorch reference for that activation\n",
        "def ActivationFunction(activation_name):\n",
        "  if(activation_name == 'relu'):\n",
        "    return F.relu  \n",
        "  elif(activation_name == 'elu'):\n",
        "    return F.elu\n",
        "  elif(activation_name == 'sigmoid'):\n",
        "    return F.sigmoid\n",
        "  elif(activation_name == 'gelu'):\n",
        "    return F.gelu\n",
        "  else:\n",
        "    return None"
      ],
      "metadata": {
        "id": "md4p1DgOJhRH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CnnModel(nn.Module):\n",
        "  def __init__(self, conv_attributes, pool_attributes,in_feature,activation_name,batch_normalization,dropout,dense_layer_size):\n",
        "    super(CnnModel, self).__init__()\n",
        "\n",
        "    self.batch_normalization = batch_normalization #batch normalization \n",
        "    self.dropout = dropout # dropout\n",
        "    #First Convolution and Pooling Layer\n",
        "    self.conv1= nn.Conv2d(conv_attributes[0][\"in_channels\"], conv_attributes[0][\"out_channels\"], conv_attributes[0][\"kernel_size\"])\n",
        "    self.bn1 = nn.BatchNorm2d(conv_attributes[0][\"out_channels\"])\n",
        "    self.act1 = ActivationFunction(activation_name)\n",
        "    self.pool1= nn.MaxPool2d(pool_attributes[0][\"kernel_size\"], pool_attributes[0][\"stride\"])\n",
        "\n",
        "    #Second Convolution and Pooling Layer\n",
        "    self.conv2= nn.Conv2d(conv_attributes[1][\"in_channels\"], conv_attributes[1][\"out_channels\"], conv_attributes[1][\"kernel_size\"])\n",
        "    self.bn2 = nn.BatchNorm2d(conv_attributes[1][\"out_channels\"])\n",
        "    self.act2 = ActivationFunction(activation_name)\n",
        "    self.pool2= nn.MaxPool2d(pool_attributes[1][\"kernel_size\"], pool_attributes[1][\"stride\"])\n",
        "\n",
        "    #Third Convolution and Pooling Layer\n",
        "    self.conv3= nn.Conv2d(conv_attributes[2][\"in_channels\"], conv_attributes[2][\"out_channels\"], conv_attributes[2][\"kernel_size\"])\n",
        "    self.bn3 = nn.BatchNorm2d(conv_attributes[2][\"out_channels\"])\n",
        "    self.act3 = ActivationFunction(activation_name)\n",
        "    self.pool3= nn.MaxPool2d(pool_attributes[2][\"kernel_size\"], pool_attributes[2][\"stride\"])\n",
        "\n",
        "    #Fourth Convolution and Pooling Layer\n",
        "    self.conv4= nn.Conv2d(conv_attributes[3][\"in_channels\"], conv_attributes[3][\"out_channels\"], conv_attributes[3][\"kernel_size\"])\n",
        "    self.bn4 = nn.BatchNorm2d(conv_attributes[3][\"out_channels\"])\n",
        "    self.act4 = ActivationFunction(activation_name)\n",
        "    self.pool4= nn.MaxPool2d(pool_attributes[3][\"kernel_size\"], pool_attributes[3][\"stride\"])\n",
        "\n",
        "    #Fifth Convolution and Pooling Layer\n",
        "    self.conv5= nn.Conv2d(conv_attributes[4][\"in_channels\"], conv_attributes[4][\"out_channels\"], conv_attributes[4][\"kernel_size\"])\n",
        "    self.bn5 = nn.BatchNorm2d(conv_attributes[4][\"out_channels\"])\n",
        "    self.act5 = ActivationFunction(activation_name)\n",
        "    self.pool5= nn.MaxPool2d(pool_attributes[4][\"kernel_size\"], pool_attributes[4][\"stride\"])\n",
        "\n",
        "    #First Dense Layer\n",
        "    self.fc1 = nn.Linear(in_feature, dense_layer_size)\n",
        "    self.fc1_act = ActivationFunction(activation_name)\n",
        "    self.fc2 = nn.Linear(dense_layer_size, 10)\n",
        "\n",
        "  def forward(self,x):\n",
        "    if self.batch_normalization:\n",
        "      x = self.pool1(self.act1(self.bn1(self.conv1(x)))) #First block of layer containing one conv layer with batch normalization and activation function followed by one pooling layer\n",
        "      x = self.pool2(self.act2(self.bn2(self.conv2(x)))) #Second block of layer containing one conv layer with batch normalization and activation function followed by one pooling layer\n",
        "      x = self.pool3(self.act3(self.bn3(self.conv3(x)))) #Third block of layer containing one conv layer with batch normalization and activation function followed by one pooling layer\n",
        "      x = self.pool4(self.act4(self.bn4(self.conv4(x)))) #Fourth block of layer containing one conv layer with batch normalization and activation function followed by one pooling layer\n",
        "      x = self.pool5(self.act5(self.bn5(self.conv5(x)))) #Fifth block of layer containing one conv layer with batch normalization and activation function followed by one pooling layer\n",
        "    else:\n",
        "      x = self.pool1(self.act1(self.conv1(x))) #First block of layer containing one conv layer with  activation function followed by one pooling layer\n",
        "      x = self.pool2(self.act2(self.conv2(x))) #Second block of layer containing one conv layer with  activation function followed by one pooling layer\n",
        "      x = self.pool3(self.act3(self.conv3(x))) #Third block of layer containing one conv layer with  activation function followed by one pooling layer\n",
        "      x = self.pool4(self.act4(self.conv4(x))) #Fourth block of layer containing one conv layer with  activation function followed by one pooling layer\n",
        "      x = self.pool5(self.act5(self.conv5(x))) #Fifth block of layer containing one conv layer with  activation function followed by one pooling layer\n",
        "\n",
        "    x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "    print(x.shape)\n",
        "    x = self.fc1(x)\n",
        "    x = self.fc1_act(x)\n",
        "    x = nn.Dropout(self.dropout)(x)\n",
        "    x = self.fc2(x)\n",
        "    x = F.softmax(x,dim=1)                     \n",
        "    return x"
      ],
      "metadata": {
        "id": "-fhgXbvhiExl"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training Function\n",
        "def TrainNetwork(model,num_epochs, batch_size,learning_rate,optimizer_name,weight_decay,resized_shape,actual_data_path,dataset_augmentation,wandb_fn):\n",
        "  print(\"CHECK---0.1\")\n",
        "  loss_funt = LossFunction() #Loss function is called\n",
        "  optimizer = OptimizerFunction(model, learning_rate, weight_decay, optimizer_name) #Optimization function is called\n",
        "  print(\"CHECK---1\")\n",
        "  #Calling Compose returns a transform function that performs image transformation.\n",
        "  train_transforms = A.Compose([\n",
        "            A.Resize(resized_shape,resized_shape),\n",
        "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "            ToTensorV2(),])\n",
        "  \n",
        "  if dataset_augmentation:\n",
        "    print(\"CHECK---2\")\n",
        "    augmented_transforms = A.Compose([A.SmallestMaxSize(max_size=350),\n",
        "              A.Resize(resized_shape,resized_shape),\n",
        "              A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=360, p=0.5),\n",
        "              A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n",
        "              A.RandomBrightnessContrast(p=0.5),\n",
        "              A.MultiplicativeNoise(multiplier=[0.5,2], per_channel=True, p=0.2),\n",
        "              A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "              A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
        "              A.RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
        "              ToTensorV2(),])\n",
        "  \n",
        "  #Function to create train, validation dataset and returns the train and validation image paths\n",
        "  train_image_paths, valid_image_paths=CreateTrainDataset(actual_data_path)\n",
        "  print(\"CHECK---3\")\n",
        "  #Training Dataset created with train_transforms\n",
        "  train_dataset = iNaturalist_12KDataset(train_image_paths,train_transforms)\n",
        "  print(\"CHECK---4\")\n",
        "  if dataset_augmentation:\n",
        "    transformed_dataset = iNaturalist_12KDataset(train_image_paths,augmented_transforms)   #Transformed Dataset created with augmented_transforms\n",
        "    train_dataset = torch.utils.data.ConcatDataset([transformed_dataset,train_dataset])\n",
        "  print(\"CHECK---5\")\n",
        "\n",
        "  #Validation Dataset created\n",
        "  valid_dataset = iNaturalist_12KDataset(valid_image_paths,train_transforms) #train transforms are applied\n",
        "  #Dataloader loads train dataset\n",
        "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  #Dataloader loads validation dataset\n",
        "  valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "  print(\"CHECK---6\")\n",
        "  #Training the network\n",
        "  n_total_steps = len(train_loader)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    cumulative_loss = 0  \n",
        "    correct_training = 0  \n",
        "    model.train(True) # For training\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "      if enable_GPU == 1 :\n",
        "        images = images.to(Device)\n",
        "        labels = labels.to(Device)\n",
        "\n",
        "      # Forward pass\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      correct_training += (predicted == labels).sum().item()\n",
        "      loss = loss_funt(outputs, labels)\n",
        "\n",
        "      # Backward and optimize\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      cumulative_loss += loss.item()\n",
        "      if (i+1) % 30 == 0:\n",
        "        print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    if wandb_fn:\n",
        "      wandb.log({\"Epoch\":epoch, \"Training Loss\": cumulative_loss/n_total_steps, \"Training Accuracy\": correct_training/n_total_steps })\n",
        "    print('Finished Training---------------------')\n",
        "\n",
        "    #Validating the trained model\n",
        "    n_valid_steps = len(valid_loader)\n",
        "    model.train(False)\n",
        "    correct_validation = 0\n",
        "    for i, (images, labels) in enumerate(valid_loader):\n",
        "      if enable_GPU == 1 :\n",
        "        images = images.to(Device)\n",
        "        labels = labels.to(Device)\n",
        "    # Forward pass\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      if enable_GPU == 1 :\n",
        "        predicted = predicted.cpu()\n",
        "      correct_validation += (predicted == labels).sum().item()\n",
        "    if wandb_fn:\n",
        "        wandb.log({\"Validation Accuracy\": correct_validation/n_valid_steps})\n",
        "\n",
        "    print(\"Train Accuracy in Epoch {0}/{1} = {2} : \".format(epoch+1 , num_epochs , correct_training/n_total_steps)) \n",
        "    print(\"Val Accuracy in Epoch {0}/{1} = {2} : \".format(epoch+1 , num_epochs , correct_validation/n_valid_steps))  \n",
        "    print(\"Loss in Epoch {0}/{1} = {2} : \".format(epoch+1 , num_epochs , cumulative_loss/n_total_steps))  \n",
        "    print()\n",
        "\n"
      ],
      "metadata": {
        "id": "k8ku7nlNhwlc"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SaveModel(model):\n",
        "  PATH = '/content/drive/MyDrive/cnn.pth'\n",
        "  torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "UGa9Z_y5wlhr"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def TestNetwork(model,num_epochs, batch_size,learning_rate,resized_shape,actual_data_path):\n",
        "  with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    n_class_correct = [0 for i in range(10)]\n",
        "    n_class_samples = [0 for i in range(10)]\n",
        "\n",
        "    #Function for image augmentation.Calling Compose returns a transform function that performs image augmentation.\n",
        "    test_transforms = A.Compose([# A.SmallestMaxSize(max_size=350),\n",
        "          # A.CenterCrop(height=256, width=256),\n",
        "          A.Resize(resized_shape,resized_shape),\n",
        "          A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "          ToTensorV2()])\n",
        "    \n",
        "    #Function to create test dataset and returns the test image paths\n",
        "    test_image_paths=CreateTestDataset(actual_data_path)\n",
        "    \n",
        "    #Test Dataset created\n",
        "    test_dataset = iNaturalist_12KDataset(test_image_paths,test_transforms)\n",
        "\n",
        "    #Dataloader loads test dataset\n",
        "    test_loader = DataLoader(\n",
        "    test_dataset, batch_size=batch_size, shuffle=False)\n",
        "        \n",
        "    for images, labels in test_loader:\n",
        "      if enable_GPU == 1:\n",
        "        images = images.to(Device)\n",
        "        labels = labels.to(Device)\n",
        "      outputs = model(images)\n",
        "      # max returns (value ,index)\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      n_samples += labels.size(0)\n",
        "      n_correct += (predicted == labels).sum().item()\n",
        "      for i in range(predicted.size()[0]):\n",
        "        label = labels[i]\n",
        "        pred = predicted[i]\n",
        "        if (label == pred):\n",
        "            n_class_correct[label] += 1\n",
        "        n_class_samples[label] += 1\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network: {acc} %')\n",
        "\n",
        "    for i in range(10):\n",
        "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
        "        print(f'Accuracy of {classes[i]}: {acc} %')"
      ],
      "metadata": {
        "id": "2NfP-wztztIv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Calculates the input feature for the dense linear layer\n",
        "def LinearInFeatureCalculate(initial_dim,conv_attributes,pool_attributes):\n",
        "  for i in range(5):\n",
        "    D = (initial_dim + 2*conv_attributes[i][\"padding\"] - conv_attributes[i][\"dilation\"]*(conv_attributes[i][\"kernel_size\"]-1) - 1)//(conv_attributes[i][\"stride\"]) + 1\n",
        "    D = (D - pool_attributes[i][\"kernel_size\"])//(pool_attributes[i][\"stride\"]) + 1\n",
        "    initial_dim = D\n",
        "  return D\n"
      ],
      "metadata": {
        "id": "lBScrogGDsZv"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sweep Config**"
      ],
      "metadata": {
        "id": "fCPdk3H8AqjZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AduMYL7Jd8_V"
      },
      "outputs": [],
      "source": [
        "#This is the config file for performing sweep in wandb\n",
        "sweep_config = {\n",
        "  'name': 'Assignment2_PartA_Q2',\n",
        "  'method': 'bayes',\n",
        "  'metric': {\n",
        "      'name': 'Validation Accuracy',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "  'parameters': {\n",
        "      'epochs': {\n",
        "            'values': [1,5,10,15,20,30]\n",
        "        },\n",
        "        'conv_attributes_channels': {\n",
        "            'values': [[32,64,32,64,32],[32,32,32,32,32],[16,32,64,128,256],[32,64,128,256,512],[256,128,64,32,16],[64,64,64,64,64],[64,128,256,512,1024]]\n",
        "        },\n",
        "        'conv_attributes_kernel_size': {\n",
        "            'values': [[3,3,5,7,9],[7,5,5,3,3],[11,7,5,3,3],[3,3,3,5,5],[3,3,3,3,3],[11,7,7,5,3],[11,9,7,5,3],[3,5,7,9,11]]\n",
        "        },\n",
        "        'pool_attributes_kernel_size': {\n",
        "            'values': [[2,2,2,2,2],[2,2,2,1,1],[2,1,3,1,2],[3,3,3,2,2]]\n",
        "        },\n",
        "        'pool_attributes_stride': {\n",
        "            'values': [[2,2,2,2,2],[2,2,2,1,1],[1,1,2,2,2],[1,2,1,2,1],[2,2,2,2,1]]\n",
        "        },\n",
        "        'dense_layer_size': {\n",
        "            'values': [32,64,128,256,512]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [0.001,0.002,0.0015,0.0001,0.00015, 0.00001]\n",
        "        },\n",
        "        'activation': {\n",
        "            'values': ['sigmoid', 'relu','elu','gelu']\n",
        "        },\n",
        "        'dropout': {\n",
        "            'values': [0.0 ,0.2 ,0.3 ,0.4 ,0.5]\n",
        "        },\n",
        "        'batch_normalization': {\n",
        "            'values': [True, False]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [16, 32, 64]\n",
        "        },\n",
        "        'weight_decay': {\n",
        "            'values': [0.0,0.00001,0.0001]\n",
        "        },\n",
        "        'dataset_augmentation':{\n",
        "              'values': [True , False]\n",
        "        },\n",
        "        'optimizer_name':{\n",
        "              'values': ['Adam' , 'SGD']\n",
        "        }\n",
        "    }\n",
        "}\n",
        "sweep_id = wandb.sweep(sweep_config,entity=\"cs21s045_cs21s011\",project=\"Assignment2_PartA\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This function needs to be passed to sweep_agent\n",
        "def train_wandb():\n",
        "  run = wandb.init()\n",
        "  config = run.config\n",
        "\n",
        "  print(\"Hello\")\n",
        "  resized_shape = 256\n",
        "\n",
        "\n",
        "  ##Hyper-parameters of the model training like number of epochs, batch size, learning rate etc from sweep\n",
        "  num_epochs=config.epochs\n",
        "  batch_size=config.batch_size\n",
        "  learning_rate=config.learning_rate\n",
        "  optimizer_name = config.optimizer_name\n",
        "  weight_decay=config.weight_decay\n",
        "  #Select the activation function\n",
        "  activation_name = config.activation\n",
        "  #Batch Normalization\n",
        "  batch_normalization = config.batch_normalization\n",
        "  #Dropout used\n",
        "  dropout = config.dropout\n",
        "  #dense layer size\n",
        "  dense_layer_size = config.dense_layer_size\n",
        "  #For data augmentation\n",
        "  dataset_augmentation = config.dataset_augmentation\n",
        "\n",
        "  actual_data_path = \"/content/drive/MyDrive/inaturalist_12K\"\n",
        "\n",
        "  conv_attributes = [{\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1},\n",
        "                     {\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1},\n",
        "                     {\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1},\n",
        "                     {\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1},\n",
        "                     {\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1}]\n",
        "  \n",
        "  \n",
        "  ##Attributes for 1st Convolution Layer\n",
        "  conv_attributes[0][\"in_channels\"]=3\n",
        "  conv_attributes[0][\"out_channels\"]=config.conv_attributes_channels[0]\n",
        "  conv_attributes[0][\"kernel_size\"]=config.conv_attributes_kernel_size[0]\n",
        "\n",
        "  ##Attributes for 2nd Convolution Layer\n",
        "  conv_attributes[1][\"in_channels\"]=conv_attributes[0][\"out_channels\"]\n",
        "  conv_attributes[1][\"out_channels\"]=config.conv_attributes_channels[1]\n",
        "  conv_attributes[1][\"kernel_size\"]=config.conv_attributes_kernel_size[1]\n",
        "\n",
        "  ##Attributes for 3rd Convolution Layer\n",
        "  conv_attributes[2][\"in_channels\"]=conv_attributes[1][\"out_channels\"]\n",
        "  conv_attributes[2][\"out_channels\"]=config.conv_attributes_channels[2]\n",
        "  conv_attributes[2][\"kernel_size\"]=config.conv_attributes_kernel_size[2]\n",
        "\n",
        "  ##Attributes for 4th Convolution Layer\n",
        "  conv_attributes[3][\"in_channels\"]=conv_attributes[2][\"out_channels\"]\n",
        "  conv_attributes[3][\"out_channels\"]=config.conv_attributes_channels[3]\n",
        "  conv_attributes[3][\"kernel_size\"]=config.conv_attributes_kernel_size[3]\n",
        "\n",
        "  ##Attributes for 5th Convolution Layer\n",
        "  conv_attributes[4][\"in_channels\"]=conv_attributes[3][\"out_channels\"]\n",
        "  conv_attributes[4][\"out_channels\"]=config.conv_attributes_channels[4]\n",
        "  conv_attributes[4][\"kernel_size\"]=config.conv_attributes_kernel_size[4]\n",
        "\n",
        "  pool_attributes = [{\"kernel_size\":1, \"stride\": 1},\n",
        "                     {\"kernel_size\":1, \"stride\": 1},\n",
        "                     {\"kernel_size\":1, \"stride\": 1},\n",
        "                     {\"kernel_size\":1, \"stride\": 1},\n",
        "                     {\"kernel_size\":1, \"stride\": 1}]\n",
        "\n",
        "  ##Attributes for 1st Pooling Layer\n",
        "  pool_attributes[0][\"kernel_size\"]=config.pool_attributes_kernel_size[0]\n",
        "  pool_attributes[0][\"stride\"]=config.pool_attributes_stride[0]\n",
        "\n",
        "  ##Attributes for 2nd Pooling Layer\n",
        "  pool_attributes[1][\"kernel_size\"]=config.pool_attributes_kernel_size[1]\n",
        "  pool_attributes[1][\"stride\"]=config.pool_attributes_stride[1]\n",
        "  \n",
        "  ##Attributes for 3rd Pooling Layer\n",
        "  pool_attributes[2][\"kernel_size\"]=config.pool_attributes_kernel_size[2]\n",
        "  pool_attributes[2][\"stride\"]=config.pool_attributes_stride[2]\n",
        "\n",
        "  ##Attributes for 4th Pooling Layer\n",
        "  pool_attributes[3][\"kernel_size\"]=config.pool_attributes_kernel_size[3]\n",
        "  pool_attributes[3][\"stride\"]=config.pool_attributes_stride[3]\n",
        "\n",
        "  ##Attributes for 5th Pooling Layer\n",
        "  pool_attributes[4][\"kernel_size\"]=config.pool_attributes_kernel_size[4]\n",
        "  pool_attributes[4][\"stride\"]=config.pool_attributes_stride[4]\n",
        "\n",
        " ##Calculating the input dimension for the Dense Linear layer\n",
        "  final_dim=LinearInFeatureCalculate(resized_shape,conv_attributes,pool_attributes) #height,width of the dense layer\n",
        "  in_feature = (final_dim ** 2) * conv_attributes[4][\"out_channels\"] #number of input nodes in the dense layer\n",
        "  print(in_feature)\n",
        "\n",
        "  \n",
        "  #If the enable_GPU flag is on then the run will use GPU\n",
        "  if enable_GPU == 1:\n",
        "    model = CnnModel(conv_attributes, pool_attributes,in_feature,activation_name,batch_normalization,dropout,dense_layer_size).to(Device)\n",
        "  else :\n",
        "    model = CnnModel(conv_attributes, pool_attributes,in_feature,activation_name,batch_normalization,dropout,dense_layer_size)\n",
        "  print(model)\n",
        "\n",
        "  #Function for training the model with parameters model,num_epochs, batch_size,learning_rate,optimizer_name\n",
        "  TrainNetwork(model,num_epochs, batch_size,learning_rate,optimizer_name,weight_decay,resized_shape,actual_data_path,dataset_augmentation,True)\n",
        "  \n",
        "  #Deleting the model after use\n",
        "  del model\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "  "
      ],
      "metadata": {
        "id": "LzOiIdYQyM7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run this cell to start sweep\n",
        "wandb.agent(sweep_id, train_wandb , project=\"Assignment2_PartA\",count=50)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "CTJeBPEF6XVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main function**"
      ],
      "metadata": {
        "id": "KS9k_0LHD9Y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Main function to build train and test the model without sweep\n",
        "def main():\n",
        "  print(\"Hello\")\n",
        "  resized_shape = 256\n",
        "\n",
        "  ##Hyper-parameters of the model training like number of epochs, batch size, learning rate\n",
        "  num_epochs=2\n",
        "  batch_size=64\n",
        "  learning_rate=0.001\n",
        "  optimizer_name = \"Adam\"\n",
        "  weight_decay=0.0\n",
        "  #For data augmentation\n",
        "  dataset_augmentation = True\n",
        "\n",
        "  actual_data_path = \"/content/drive/MyDrive/inaturalist_12K\"\n",
        "\n",
        "  conv_attributes = [{\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1},\n",
        "                     {\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1},\n",
        "                     {\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1},\n",
        "                     {\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1},\n",
        "                     {\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1}]\n",
        "  \n",
        "  \n",
        "  ##Attributes for 1st Convolution Layer\n",
        "  conv_attributes[0][\"in_channels\"]=3\n",
        "  conv_attributes[0][\"out_channels\"]=6\n",
        "  conv_attributes[0][\"kernel_size\"]=11\n",
        "\n",
        "  ##Attributes for 2nd Convolution Layer\n",
        "  conv_attributes[1][\"in_channels\"]=conv_attributes[0][\"out_channels\"]\n",
        "  conv_attributes[1][\"out_channels\"]=12\n",
        "  conv_attributes[1][\"kernel_size\"]=9\n",
        "\n",
        "  ##Attributes for 3rd Convolution Layer\n",
        "  conv_attributes[2][\"in_channels\"]=conv_attributes[1][\"out_channels\"]\n",
        "  conv_attributes[2][\"out_channels\"]=16\n",
        "  conv_attributes[2][\"kernel_size\"]=7\n",
        "\n",
        "  ##Attributes for 4th Convolution Layer\n",
        "  conv_attributes[3][\"in_channels\"]=conv_attributes[2][\"out_channels\"]\n",
        "  conv_attributes[3][\"out_channels\"]=32\n",
        "  conv_attributes[3][\"kernel_size\"]=5\n",
        "\n",
        "  ##Attributes for 5th Convolution Layer\n",
        "  conv_attributes[4][\"in_channels\"]=conv_attributes[3][\"out_channels\"]\n",
        "  conv_attributes[4][\"out_channels\"]=32\n",
        "  conv_attributes[4][\"kernel_size\"]=3\n",
        "\n",
        "  pool_attributes = [{\"kernel_size\":1, \"stride\": 1},\n",
        "                     {\"kernel_size\":1, \"stride\": 1},\n",
        "                     {\"kernel_size\":1, \"stride\": 1},\n",
        "                     {\"kernel_size\":1, \"stride\": 1},\n",
        "                     {\"kernel_size\":1, \"stride\": 1}]\n",
        "\n",
        "  ##Attributes for 1st Pooling Layer\n",
        "  pool_attributes[0][\"kernel_size\"]=3\n",
        "  pool_attributes[0][\"stride\"]=2\n",
        "\n",
        "  ##Attributes for 2nd Pooling Layer\n",
        "  pool_attributes[1][\"kernel_size\"]=3\n",
        "  pool_attributes[1][\"stride\"]=2\n",
        "  \n",
        "  ##Attributes for 3rd Pooling Layer\n",
        "  pool_attributes[2][\"kernel_size\"]=3\n",
        "  pool_attributes[2][\"stride\"]=2\n",
        "\n",
        "  ##Attributes for 4th Pooling Layer\n",
        "  pool_attributes[3][\"kernel_size\"]=2\n",
        "  pool_attributes[3][\"stride\"]=2\n",
        "\n",
        "  ##Attributes for 5th Pooling Layer\n",
        "  pool_attributes[4][\"kernel_size\"]=2\n",
        "  pool_attributes[4][\"stride\"]=2\n",
        "\n",
        " ##Calculating the input dimension for the Dense Linear layer\n",
        "  final_dim=LinearInFeatureCalculate(resized_shape,conv_attributes,pool_attributes) #height,width of the dense layer\n",
        "  in_feature = (final_dim ** 2) * conv_attributes[4][\"out_channels\"] #number of input nodes in the dense layer\n",
        "  print(in_feature)\n",
        "\n",
        "  #Select the activation function\n",
        "  activation_name = 'relu'\n",
        "  #Batch Normalization\n",
        "  batch_normalization = True\n",
        "  #Dropout used\n",
        "  dropout = 0\n",
        "  #dense layer size\n",
        "  dense_layer_size = 32\n",
        "  #If the enable_GPU flag is on then the run will use GPU\n",
        "  if enable_GPU == 1:\n",
        "    model = CnnModel(conv_attributes, pool_attributes,in_feature,activation_name,batch_normalization,dropout,dense_layer_size).to(Device)\n",
        "  else :\n",
        "    model = CnnModel(conv_attributes, pool_attributes,in_feature,activation_name,batch_normalization,dropout,dense_layer_size)\n",
        "  print(model)\n",
        "  print(\"CHECK---0\")\n",
        "  #Function for training the model with parameters model,num_epochs, batch_size,learning_rate,optimizer_name\n",
        "  TrainNetwork(model,num_epochs, batch_size,learning_rate,optimizer_name,weight_decay,resized_shape,actual_data_path,dataset_augmentation,True)\n",
        "\n",
        "  #Function for testing the model accuracy on the test data with parameters model,num_epochs, batch_size,learning_rate\n",
        "  #TestNetwork(model,num_epochs, batch_size,learning_rate,resized_shape,actual_data_path)"
      ],
      "metadata": {
        "id": "pejeGIEMD87w"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if  __name__ ==\"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "id": "dxEPk_H5FT4_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b463363b-7d20-499d-ce31-3f230a56e344"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "512\n",
            "CnnModel(\n",
            "  (conv1): Conv2d(3, 6, kernel_size=(11, 11), stride=(1, 1))\n",
            "  (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 12, kernel_size=(9, 9), stride=(1, 1))\n",
            "  (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv2d(12, 16, kernel_size=(7, 7), stride=(1, 1))\n",
            "  (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv4): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (bn4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (bn5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=512, out_features=32, bias=True)\n",
            "  (fc2): Linear(in_features=32, out_features=10, bias=True)\n",
            ")\n",
            "CHECK---0\n",
            "CHECK---0.1\n",
            "CnnModel(\n",
            "  (conv1): Conv2d(3, 6, kernel_size=(11, 11), stride=(1, 1))\n",
            "  (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 12, kernel_size=(9, 9), stride=(1, 1))\n",
            "  (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv2d(12, 16, kernel_size=(7, 7), stride=(1, 1))\n",
            "  (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv4): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (bn4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (bn5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=512, out_features=32, bias=True)\n",
            "  (fc2): Linear(in_features=32, out_features=10, bias=True)\n",
            ") 0.001 0.0 Adam\n",
            "CHECK---1\n",
            "CHECK---2\n",
            "CHECK---3\n",
            "CHECK---4\n",
            "CHECK---5\n",
            "CHECK---6\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 512])\n",
            "torch.Size([64, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BwRSrf-_hDeW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}