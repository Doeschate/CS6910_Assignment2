{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**ASSIGNMENT-2**","metadata":{"id":"i9oMWDx10UUx"}},{"cell_type":"markdown","source":"Learn how to use CNNs: train from scratch, finetune a pretrained model, use a pre-trained model as it is.\n","metadata":{"id":"CQnccrq2ewXw"}},{"cell_type":"markdown","source":"**Installs**","metadata":{"id":"uifpKNB1odVq"}},{"cell_type":"code","source":"!pip install -U albumentations\n!pip install \"opencv-python-headless<4.3\" #for import albumentations as A\n!pip install wandb #To install wandb and evaluate models","metadata":{"id":"EAGlgBb8okJh","outputId":"791f2ce5-62d4-42dd-f29a-eec043a5479d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Imports**","metadata":{"id":"Vfgl_cijfI9g"}},{"cell_type":"code","source":"import os\nimport copy\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models,datasets,transforms\nimport torchvision\n\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport cv2\nimport glob\nimport numpy as np\nimport random\nimport wandb\nimport gc\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nimport matplotlib.pyplot as plt\nfrom itertools import chain\nenable_GPU = 0","metadata":{"id":"qpDHokXY8ffl","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Enabling GPU**","metadata":{"id":"cBA2e2NrmwYD"}},{"cell_type":"code","source":"Device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(torch.cuda.get_device_name(0))\nenable_GPU = 1","metadata":{"id":"UajdMgmgn3MR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Download iNaturalist-12K dataset**","metadata":{"id":"3i4cMnGWFdrG"}},{"cell_type":"code","source":"!wget https://storage.googleapis.com/wandb_datasets/nature_12K.zip\n!unzip nature_12K.zip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -r nature_12K.zip\nactual_data_path = \"./inaturalist_12K\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Dataset Creating**","metadata":{"id":"DgxaBE_29k3U"}},{"cell_type":"code","source":"# get all the paths from train_data_path and returns image paths for train and validation set\ndef CreateTrainDataset(actual_data_path):\n    train_data_path = os.path.join(actual_data_path, \"train\")\n    train_image_paths = [] #to store image paths in list\n    classes = [] #to store class values\n    for data_path in glob.glob(train_data_path + \"/*\"):\n        train_image_paths.append(glob.glob(data_path + '/*')) #stores all the training image paths in this list\n    train_image_paths = list(chain.from_iterable(train_image_paths))\n    random.shuffle(train_image_paths)\n\n    # split train valid from train paths (90,10)\n    train_image_paths, valid_image_paths = train_image_paths[:int(0.9*len(train_image_paths))], train_image_paths[int(0.9*len(train_image_paths)):] \n    return train_image_paths, valid_image_paths\n\n# create the test_image_paths\ndef CreateTestDataset(actual_data_path):\n    test_data_path = os.path.join(actual_data_path, \"val\")\n    test_image_paths = []\n    for data_path in glob.glob(test_data_path + '/*'):\n        test_image_paths.append(glob.glob(data_path + '/*')) #stores all the test images path in this list\n    test_image_paths = list(chain.from_iterable(test_image_paths))\n    return test_image_paths","metadata":{"id":"ora_zBPn7o1i","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create dictionary for class indexes\ntrain_data_path = os.path.join(actual_data_path, \"train\")\nclasses = [] #to store class values\nfor data_path in glob.glob(train_data_path + \"/*\"):\n    classes.append(data_path.split('/')[-1])\nidx_to_class = {i:j for i, j in enumerate(classes)} #index to class map\nclass_to_idx = {value:key for key,value in idx_to_class.items()} #class to index map","metadata":{"id":"8CxrpoqISjgI","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Dataset Class**","metadata":{"id":"-B7uy3GTY1gi"}},{"cell_type":"code","source":"#Function returns images and corresponding lebels after performing transforms\nclass iNaturalist_12KDataset(Dataset):\n    def __init__(self, image_paths, transform=False):\n        self.image_paths = image_paths\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image_filepath = self.image_paths[idx]\n        image = cv2.imread(image_filepath)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        label = image_filepath.split('/')[-2]\n        label = class_to_idx[label]\n        if self.transform is not None:\n            image = self.transform(image=image)[\"image\"]\n        return image, label","metadata":{"id":"gZs1aTpeVCst","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Building the Model**","metadata":{"id":"y53VI2iPpO7k"}},{"cell_type":"code","source":"#Optimization Function\ndef OptimizerFunction(model, learning_rate,weight_decay, optimizer_name):\n    if optimizer_name == \"SGD\":\n        opt = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    elif optimizer_name == \"Adam\":\n        opt = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()\n    return opt\n    ","metadata":{"id":"5trpZKH-nIxp","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loss Function\ndef LossFunction():\n    return nn.CrossEntropyLoss()","metadata":{"id":"kb7hUIJfxQHX","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Activation Function\n#To add another activation just add another else statement for that activation and return the corresponding pytorch reference for that activation\ndef ActivationFunction(activation_name):\n    if(activation_name == 'relu'):\n        return F.relu  \n    elif(activation_name == 'elu'):\n        return F.elu\n    elif(activation_name == 'sigmoid'):\n        return F.sigmoid\n    elif(activation_name == 'gelu'):\n        return F.gelu\n    else:\n        return None","metadata":{"id":"md4p1DgOJhRH","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CnnModel(nn.Module):\n    def __init__(self, conv_attributes, pool_attributes,in_feature,activation_name,batch_normalization,dropout,dense_layer_size):\n        super(CnnModel, self).__init__()\n\n        self.batch_normalization = batch_normalization #batch normalization \n        self.dropout = dropout # dropout\n        #First Convolution and Pooling Layer\n        self.conv1= nn.Conv2d(conv_attributes[0][\"in_channels\"], conv_attributes[0][\"out_channels\"], conv_attributes[0][\"kernel_size\"])\n        self.bn1 = nn.BatchNorm2d(conv_attributes[0][\"out_channels\"])\n        self.act1 = ActivationFunction(activation_name)\n        self.pool1= nn.MaxPool2d(pool_attributes[0][\"kernel_size\"], pool_attributes[0][\"stride\"])\n\n        #Second Convolution and Pooling Layer\n        self.conv2= nn.Conv2d(conv_attributes[1][\"in_channels\"], conv_attributes[1][\"out_channels\"], conv_attributes[1][\"kernel_size\"])\n        self.bn2 = nn.BatchNorm2d(conv_attributes[1][\"out_channels\"])\n        self.act2 = ActivationFunction(activation_name)\n        self.pool2= nn.MaxPool2d(pool_attributes[1][\"kernel_size\"], pool_attributes[1][\"stride\"])\n\n        #Third Convolution and Pooling Layer\n        self.conv3= nn.Conv2d(conv_attributes[2][\"in_channels\"], conv_attributes[2][\"out_channels\"], conv_attributes[2][\"kernel_size\"])\n        self.bn3 = nn.BatchNorm2d(conv_attributes[2][\"out_channels\"])\n        self.act3 = ActivationFunction(activation_name)\n        self.pool3= nn.MaxPool2d(pool_attributes[2][\"kernel_size\"], pool_attributes[2][\"stride\"])\n\n        #Fourth Convolution and Pooling Layer\n        self.conv4= nn.Conv2d(conv_attributes[3][\"in_channels\"], conv_attributes[3][\"out_channels\"], conv_attributes[3][\"kernel_size\"])\n        self.bn4 = nn.BatchNorm2d(conv_attributes[3][\"out_channels\"])\n        self.act4 = ActivationFunction(activation_name)\n        self.pool4= nn.MaxPool2d(pool_attributes[3][\"kernel_size\"], pool_attributes[3][\"stride\"])\n\n        #Fifth Convolution and Pooling Layer\n        self.conv5= nn.Conv2d(conv_attributes[4][\"in_channels\"], conv_attributes[4][\"out_channels\"], conv_attributes[4][\"kernel_size\"])\n        self.bn5 = nn.BatchNorm2d(conv_attributes[4][\"out_channels\"])\n        self.act5 = ActivationFunction(activation_name)\n        self.pool5= nn.MaxPool2d(pool_attributes[4][\"kernel_size\"], pool_attributes[4][\"stride\"])\n\n        #First Dense Layer\n        self.fc1 = nn.Linear(in_feature, dense_layer_size)\n        self.fc1_act = ActivationFunction(activation_name)\n        self.fc2 = nn.Linear(dense_layer_size, 10)\n\n    def forward(self,x):\n        if self.batch_normalization:\n            x = self.pool1(self.act1(self.bn1(self.conv1(x)))) #First block of layer containing one conv layer with batch normalization and activation function followed by one pooling layer\n            x = self.pool2(self.act2(self.bn2(self.conv2(x)))) #Second block of layer containing one conv layer with batch normalization and activation function followed by one pooling layer\n            x = self.pool3(self.act3(self.bn3(self.conv3(x)))) #Third block of layer containing one conv layer with batch normalization and activation function followed by one pooling layer\n            x = self.pool4(self.act4(self.bn4(self.conv4(x)))) #Fourth block of layer containing one conv layer with batch normalization and activation function followed by one pooling layer\n            x = self.pool5(self.act5(self.bn5(self.conv5(x)))) #Fifth block of layer containing one conv layer with batch normalization and activation function followed by one pooling layer\n        else:\n            x = self.pool1(self.act1(self.conv1(x))) #First block of layer containing one conv layer with  activation function followed by one pooling layer\n            x = self.pool2(self.act2(self.conv2(x))) #Second block of layer containing one conv layer with  activation function followed by one pooling layer\n            x = self.pool3(self.act3(self.conv3(x))) #Third block of layer containing one conv layer with  activation function followed by one pooling layer\n            x = self.pool4(self.act4(self.conv4(x))) #Fourth block of layer containing one conv layer with  activation function followed by one pooling layer\n            x = self.pool5(self.act5(self.conv5(x))) #Fifth block of layer containing one conv layer with  activation function followed by one pooling layer\n\n        x = torch.flatten(x, 1) # flatten all dimensions except batch\n        x = self.fc1(x)\n        x = self.fc1_act(x)\n        x = nn.Dropout(self.dropout)(x)\n        x = self.fc2(x)\n        x = F.softmax(x,dim=1)                     \n        return x","metadata":{"id":"-fhgXbvhiExl","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training Function\ndef TrainNetwork(model,num_epochs, batch_size,learning_rate,optimizer_name,weight_decay,resized_shape,actual_data_path,dataset_augmentation,wandb_fn):\n    loss_funt = LossFunction() #Loss function is called\n    optimizer = OptimizerFunction(model, learning_rate, weight_decay, optimizer_name) #Optimization function is called\n    #Calling Compose returns a transform function that performs image transformation.\n    train_transforms = A.Compose([A.Resize(resized_shape,resized_shape),A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),ToTensorV2()])\n  \n    if dataset_augmentation:\n        augmented_transforms = A.Compose([A.SmallestMaxSize(max_size=350),\n              A.Resize(resized_shape,resized_shape),\n              A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=360, p=0.5),\n              A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n              A.RandomBrightnessContrast(p=0.5),\n              A.MultiplicativeNoise(multiplier=[0.5,2], per_channel=True, p=0.2),\n              A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n              A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n              A.RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n              ToTensorV2()])\n  \n    #Function to create train, validation dataset and returns the train and validation image paths\n    train_image_paths, valid_image_paths=CreateTrainDataset(actual_data_path)\n    #Training Dataset created with train_transforms\n    train_dataset = iNaturalist_12KDataset(train_image_paths,train_transforms)\n    if dataset_augmentation:\n        transformed_dataset = iNaturalist_12KDataset(train_image_paths,augmented_transforms)   #Transformed Dataset created with augmented_transforms\n        train_dataset = torch.utils.data.ConcatDataset([transformed_dataset,train_dataset])\n    #Validation Dataset created\n    valid_dataset = iNaturalist_12KDataset(valid_image_paths,train_transforms) #train transforms are applied\n    #Dataloader loads train dataset\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    #Dataloader loads validation dataset\n    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n    #Training the network\n    n_total_steps = len(train_loader)\n    total_data_size = len(train_loader.dataset)\n    for epoch in range(num_epochs):\n        cumulative_loss = 0  \n        correct_training = 0  \n        model.train(True) # For training\n        for i, (images, labels) in enumerate(train_loader):\n            if enable_GPU == 1 :\n                images = images.to(Device)\n                labels = labels.to(Device)\n\n            # Forward pass\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            correct_training += (predicted == labels).sum().item()\n            loss = loss_funt(outputs, labels)\n\n            # Backward and optimize\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            cumulative_loss += loss.item()\n            if (i+1) % 30 == 0:\n                print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n\n        if wandb_fn:\n            wandb.log({\"Epoch\":epoch, \"Training Loss\": cumulative_loss/total_data_size, \"Training Accuracy\": 100*(correct_training/total_data_size)})\n        print('Finished Training---------------------')\n\n        #Validating the trained model\n        with torch.no_grad():\n            n_valid_steps = len(valid_loader)\n            total_validation_data = len(valid_loader.dataset)\n            model.train(False)\n            correct_validation = 0\n            for i, (images, labels) in enumerate(valid_loader):\n                if enable_GPU == 1 :\n                    images = images.to(Device)\n                    labels = labels.to(Device)\n                # Forward pass\n                outputs = model(images)\n                _, predicted = torch.max(outputs.data, 1)\n    #             if enable_GPU == 1 :\n    #                 predicted = predicted.cpu()\n                correct_validation += (predicted == labels).sum().item()\n        if wandb_fn:\n            wandb.log({\"Validation Accuracy\": 100*(correct_validation/total_validation_data)})\n\n        print(\"Train Accuracy in Epoch {0}/{1} = {2} : \".format(epoch+1 , num_epochs , 100*(correct_training/total_data_size))) \n        print(\"Val Accuracy in Epoch {0}/{1} = {2} : \".format(epoch+1 , num_epochs , 100*(correct_validation/total_validation_data)))  \n        print(\"Loss in Epoch {0}/{1} = {2} : \".format(epoch+1 , num_epochs , cumulative_loss/total_data_size))  \n        print()\n    TestNetwork(model,num_epochs, batch_size,learning_rate,resized_shape,actual_data_path)\n    PlotGridOfImages(model,batch_size)\n    visualize_filters(model,batch_size)\n    #Deleting the model after use\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()","metadata":{"id":"k8ku7nlNhwlc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def SaveModel(model,path):\n    PATH = path\n    torch.save(model.state_dict(), PATH)","metadata":{"id":"UGa9Z_y5wlhr","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def TestNetwork(model,num_epochs, batch_size,learning_rate,resized_shape,actual_data_path):\n    with torch.no_grad():\n        n_correct = 0\n        n_samples = 0\n        n_class_correct = [0 for i in range(10)]\n        n_class_samples = [0 for i in range(10)]\n\n        #Function for image augmentation.Calling Compose returns a transform function that performs image augmentation.\n        test_transforms = A.Compose([A.Resize(resized_shape,resized_shape),A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),ToTensorV2()])\n\n        #Function to create test dataset and returns the test image paths\n        test_image_paths=CreateTestDataset(actual_data_path)\n\n        #Test Dataset created\n        test_dataset = iNaturalist_12KDataset(test_image_paths,test_transforms)\n\n        #Dataloader loads test dataset\n        test_loader = DataLoader(\n        test_dataset, batch_size=batch_size, shuffle=False)\n\n        for images, labels in test_loader:\n            if enable_GPU == 1:\n                images = images.to(Device)\n                labels = labels.to(Device)\n            outputs = model(images)\n            # max returns (value ,index)\n            _, predicted = torch.max(outputs, 1)\n            n_samples += labels.size(0)\n            n_correct += (predicted == labels).sum().item()\n            for i in range(predicted.size()[0]):\n                label = labels[i]\n                pred = predicted[i]\n                if (label == pred):\n                    n_class_correct[label] += 1\n                n_class_samples[label] += 1\n        acc = 100.0 * n_correct / n_samples\n        print(f'Accuracy of the network: {acc} %')\n        wandb.log({\"Test Accuracy\": acc})\n        path = './cnn_' + str(acc)+'.pth'\n\n        for i in range(10):\n            acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n            print(f'Accuracy of {classes[i]}: {acc} %')\n    #Deleting the model after use\n    SaveModel(model,path)\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()","metadata":{"id":"2NfP-wztztIv","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Calculates the input feature for the dense linear layer\ndef LinearInFeatureCalculate(initial_dim,conv_attributes,pool_attributes):\n    for i in range(5):\n        D = (initial_dim + 2*conv_attributes[i][\"padding\"] - conv_attributes[i][\"dilation\"]*(conv_attributes[i][\"kernel_size\"]-1) - 1)//(conv_attributes[i][\"stride\"]) + 1\n        D = (D - pool_attributes[i][\"kernel_size\"])//(pool_attributes[i][\"stride\"]) + 1\n        initial_dim = D\n    return D\n","metadata":{"id":"lBScrogGDsZv","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def TestNetworkFromPTH(model_path):\n    #Main function to build train and test the model without sweep\n    print(\"Hello\")\n    resized_shape = 256\n\n    ##Hyper-parameters of the model training like number of epochs, batch size, learning rate\n    num_epochs=30\n    batch_size=32\n    learning_rate=0.0001\n    optimizer_name = \"Adam\"\n    weight_decay=0.00001\n    #For data augmentation\n    dataset_augmentation = True\n\n    conv_attributes = [{\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1},\n                     {\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1},\n                     {\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1},\n                     {\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1},\n                     {\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1}]\n\n\n    ##Attributes for 1st Convolution Layer\n    conv_attributes[0][\"in_channels\"]=3\n    conv_attributes[0][\"out_channels\"]=256\n    conv_attributes[0][\"kernel_size\"]=3\n\n    ##Attributes for 2nd Convolution Layer\n    conv_attributes[1][\"in_channels\"]=conv_attributes[0][\"out_channels\"]\n    conv_attributes[1][\"out_channels\"]=128\n    conv_attributes[1][\"kernel_size\"]=3\n\n    ##Attributes for 3rd Convolution Layer\n    conv_attributes[2][\"in_channels\"]=conv_attributes[1][\"out_channels\"]\n    conv_attributes[2][\"out_channels\"]=64\n    conv_attributes[2][\"kernel_size\"]=5\n\n    ##Attributes for 4th Convolution Layer\n    conv_attributes[3][\"in_channels\"]=conv_attributes[2][\"out_channels\"]\n    conv_attributes[3][\"out_channels\"]=32\n    conv_attributes[3][\"kernel_size\"]=7\n\n    ##Attributes for 5th Convolution Layer\n    conv_attributes[4][\"in_channels\"]=conv_attributes[3][\"out_channels\"]\n    conv_attributes[4][\"out_channels\"]=16\n    conv_attributes[4][\"kernel_size\"]=9\n\n    pool_attributes = [{\"kernel_size\":1, \"stride\": 1},\n                     {\"kernel_size\":1, \"stride\": 1},\n                     {\"kernel_size\":1, \"stride\": 1},\n                     {\"kernel_size\":1, \"stride\": 1},\n                     {\"kernel_size\":1, \"stride\": 1}]\n\n    ##Attributes for 1st Pooling Layer\n    pool_attributes[0][\"kernel_size\"]=3\n    pool_attributes[0][\"stride\"]=2\n\n    ##Attributes for 2nd Pooling Layer\n    pool_attributes[1][\"kernel_size\"]=3\n    pool_attributes[1][\"stride\"]=2\n\n    ##Attributes for 3rd Pooling Layer\n    pool_attributes[2][\"kernel_size\"]=3\n    pool_attributes[2][\"stride\"]=2\n\n    ##Attributes for 4th Pooling Layer\n    pool_attributes[3][\"kernel_size\"]=2\n    pool_attributes[3][\"stride\"]=2\n\n    ##Attributes for 5th Pooling Layer\n    pool_attributes[4][\"kernel_size\"]=2\n    pool_attributes[4][\"stride\"]=1\n\n    ##Calculating the input dimension for the Dense Linear layer\n    final_dim=LinearInFeatureCalculate(resized_shape,conv_attributes,pool_attributes) #height,width of the dense layer\n    in_feature = (final_dim ** 2) * conv_attributes[4][\"out_channels\"] #number of input nodes in the dense layer\n    print(in_feature)\n\n    #Select the activation function\n    activation_name = 'gelu'\n    #Batch Normalization\n    batch_normalization = True\n    #Dropout used\n    dropout = 0.3\n    #dense layer size\n    dense_layer_size = 128\n    #If the enable_GPU flag is on then the run will use GPU\n    if enable_GPU == 1:\n        model = CnnModel(conv_attributes, pool_attributes,in_feature,activation_name,batch_normalization,dropout,dense_layer_size).to(Device)\n    else :\n        model = CnnModel(conv_attributes, pool_attributes,in_feature,activation_name,batch_normalization,dropout,dense_layer_size)\n    print(model)\n    model.load_state_dict(torch.load(model_path))\n#     model.eval()\n    #For plotting grid of sample test images as mentioned in Part-A Question 4\n    PlotGridOfImages(model,batch_size)\n    visualize_filters(model,batch_size)\n    generating_plots(model,batch_size)\n    print(\"TESTING-----------------\")\n    TestNetwork(model,num_epochs, batch_size,learning_rate,resized_shape,actual_data_path)\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Supporting function for plotting visualizations\ndef de_normalize(img):\n    mean, std = (0.485, 0.456, 0.406),(0.229, 0.224, 0.225)\n    z = img * torch.tensor(std).view(3, 1, 1)\n    z = z + torch.tensor(mean).view(3, 1, 1)\n\n    img2 = transforms.ToPILImage(mode='RGB')(z)\n    return img2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualizing filters of Conv1 layer\ndef visualize_filters(model,batch_size):\n    resized_shape=256\n    test_transforms = A.Compose([A.Resize(resized_shape,resized_shape),A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),ToTensorV2()])\n\n    #Function to create test dataset and returns the test image paths\n    test_image_paths=CreateTestDataset(actual_data_path)\n\n    #Test Dataset created\n    test_dataset = iNaturalist_12KDataset(test_image_paths,test_transforms)\n\n    #Dataloader loads test dataset\n    test_loader = DataLoader(\n    test_dataset, batch_size=batch_size, shuffle=True)\n    \n    \n    \n    for images, labels in test_loader:\n        img = images\n    \n    w = model.cpu().conv1.weight.data\n\n\n    im_test = model.cpu().conv1(img)\n\n    im_test = im_test[0]\n    im_test= im_test[None, :]\n    im_test=im_test.permute(1,0,2,3)\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()\n    grid_w = torchvision.utils.make_grid(w, nrow=16, normalize=True, scale_each=True, )\n    grid_im = torchvision.utils.make_grid(im_test, nrow=16, normalize=True, scale_each=True, )\n\n    plt.figure(figsize=(30, 30))\n    plt.title(\"Visualizationg of Filters of Conv1\")\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(grid_w.permute(1, 2, 0))\n    plt.savefig(\"filtersconv.png\")\n    \n    \n    plt.figure(figsize=(30, 30))\n    plt.title(\"Visualizationg of image after using conv1 filters\")\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(grid_im.permute(1, 2, 0))\n    plt.savefig(\"filtersimg_test.png\")\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plotting visualizations of 30 predictions\ndef PlotGridOfImages(model,batch_size):\n    #Function for image augmentation.Calling Compose returns a transform function that performs image augmentation.\n    batch_size=32\n    resized_shape=256\n    print(\"HELLO----------\")\n    print(batch_size)\n    test_transforms = A.Compose([A.Resize(resized_shape,resized_shape),A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),ToTensorV2()])\n    print(\"HELLO 1----------\")\n\n    #Function to create test dataset and returns the test image paths\n    test_image_paths=CreateTestDataset(actual_data_path)\n    print(\"HELLO 2----------\")\n\n    #Test Dataset created\n    test_dataset = iNaturalist_12KDataset(test_image_paths,test_transforms)\n    print(\"HELLO 3----------\")\n\n    #Dataloader loads test dataset\n    test_loader = DataLoader(\n    test_dataset, batch_size=batch_size, shuffle=True)\n    \n    print(\"HELLO 4----------\")\n\n    \n    for images, labels in test_loader:\n        if enable_GPU == 1:\n            images = images.to(Device)\n            labels = labels.to(Device)\n        outputs = model(images)\n\n        _, predicted = torch.max(outputs, 1)\n        break\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()\n    print(images.shape)\n    print(\"HELLO 4----------\")\n\n\n    print(\"HELLO 5----------\")\n\n    #Plotting model predictions\n    fig = plt.figure(figsize=(10,30))\n    for x in range(30):\n        ax = fig.add_subplot(10,3,x+1)\n        plt.imshow(de_normalize(images[x].cpu()))\n        plt.xlabel(\"True: \"+ idx_to_class[(labels[x].cpu()).item()])\n        plt.ylabel(\"Pred: \"+ idx_to_class[(predicted[x].cpu()).item()])\n        plt.xticks([])\n        plt.yticks([])\n    fig.tight_layout()\n    fig.savefig(\"Plots.png\")\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_path = '../input/trained-model/cnn_42.2.pth'\n# model = CnnModel(conv_attributes, pool_attributes,in_feature,activation_name,batch_normalization,dropout,dense_layer_size).to(Device)\nTestNetworkFromPTH(model_path)\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Sweep Config**","metadata":{"id":"fCPdk3H8AqjZ"}},{"cell_type":"code","source":"#This is the config file for performing sweep in wandb\nsweep_config = {\n  'name': 'Assignment2_PartA_Q2',\n  'method': 'bayes',\n  'metric': {\n      'name': 'Validation Accuracy',\n      'goal': 'maximize'   \n    },\n  'parameters': {\n      'epochs': {\n            'values': [30,40,50]\n        },\n        'conv_attributes_channels': {\n            'values': [[32,64,32,64,32],[32,32,32,32,32],[16,32,64,128,256],[32,64,128,256,512],[256,128,64,32,16],[64,64,64,64,64],[64,128,256,512,1024]]\n        },\n        'conv_attributes_kernel_size': {\n            'values': [[3,3,5,7,9],[7,5,5,3,3],[11,7,5,3,3],[3,3,3,5,5],[3,3,3,3,3],[11,7,7,5,3],[11,9,7,5,3],[3,5,7,9,11]]\n        },\n        'pool_attributes_kernel_size': {\n            'values': [[2,2,2,2,2],[2,2,2,1,1],[2,1,3,1,2],[3,3,3,2,2]]\n        },\n        'pool_attributes_stride': {\n            'values': [[2,2,2,2,2],[2,2,2,1,1],[1,1,2,2,2],[1,2,1,2,1],[2,2,2,2,1]]\n        },\n        'dense_layer_size': {\n            'values': [32,64,128,256,512]\n        },\n        'learning_rate': {\n            'values': [0.001,0.002,0.0015,0.0001,0.00015, 0.00001]\n        },\n        'activation': {\n            'values': ['relu','elu','gelu']\n        },\n        'dropout': {\n            'values': [0.0 ,0.2 ,0.3 ,0.4 ,0.5]\n        },\n        'batch_normalization': {\n            'values': [True]\n        },\n        'batch_size': {\n            'values': [16, 32, 64]\n        },\n        'weight_decay': {\n            'values': [0.0,0.00001,0.0001]\n        },\n        'dataset_augmentation':{\n              'values': [True]\n        },\n        'optimizer_name':{\n              'values': ['Adam']\n        }\n    }\n}\nsweep_id = wandb.sweep(sweep_config,entity=\"cs21s045_cs21s011\",project=\"Assignment2_PartA\")","metadata":{"id":"AduMYL7Jd8_V","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#This is the config file for performing sweep in wandb\nsweep_config = {\n  'name': 'Assignment2_PartA_Q2',\n  'method': 'bayes',\n  'metric': {\n      'name': 'Validation Accuracy',\n      'goal': 'maximize'   \n    },\n  'parameters': {\n      'epochs': {\n            'values': [30]\n        },\n        'conv_attributes_channels': {\n            'values': [[256,128,64,32,16]]\n        },\n        'conv_attributes_kernel_size': {\n            'values': [[3,3,5,7,9]]\n        },\n        'pool_attributes_kernel_size': {\n            'values': [[3,3,3,2,2]]\n        },\n        'pool_attributes_stride': {\n            'values': [[2,2,2,2,1]]\n        },\n        'dense_layer_size': {\n            'values': [128]\n        },\n        'learning_rate': {\n            'values': [0.0001]\n        },\n        'activation': {\n            'values': ['gelu']\n        },\n        'dropout': {\n            'values': [0.3]\n        },\n        'batch_normalization': {\n            'values': [True]\n        },\n        'batch_size': {\n            'values': [16]\n        },\n        'weight_decay': {\n            'values': [0.00001]\n        },\n        'dataset_augmentation':{\n              'values': [True]\n        },\n        'optimizer_name':{\n              'values': ['Adam']\n        }\n    }\n}\nsweep_id = wandb.sweep(sweep_config,entity=\"cs21s045_cs21s011\",project=\"Assignment2_PartA\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#This function needs to be passed to sweep_agent\ndef train_wandb():\n    run = wandb.init()\n    config = run.config\n    resized_shape = 256\n    ##Hyper-parameters of the model training like number of epochs, batch size, learning rate etc from sweep\n    num_epochs=config.epochs\n    batch_size=config.batch_size\n    learning_rate=config.learning_rate\n    optimizer_name = config.optimizer_name\n    weight_decay=config.weight_decay\n    #Select the activation function\n    activation_name = config.activation\n    #Batch Normalization\n    batch_normalization = config.batch_normalization\n    #Dropout used\n    dropout = config.dropout\n    #dense layer size\n    dense_layer_size = config.dense_layer_size\n    #For data augmentation\n    dataset_augmentation = config.dataset_augmentation\n    conv_attributes = [{\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1},\n                     {\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1},\n                     {\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1},\n                     {\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1},\n                     {\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1}]\n  \n  \n    ##Attributes for 1st Convolution Layer\n    conv_attributes[0][\"in_channels\"]=3\n    conv_attributes[0][\"out_channels\"]=config.conv_attributes_channels[0]\n    conv_attributes[0][\"kernel_size\"]=config.conv_attributes_kernel_size[0]\n\n    ##Attributes for 2nd Convolution Layer\n    conv_attributes[1][\"in_channels\"]=conv_attributes[0][\"out_channels\"]\n    conv_attributes[1][\"out_channels\"]=config.conv_attributes_channels[1]\n    conv_attributes[1][\"kernel_size\"]=config.conv_attributes_kernel_size[1]\n\n    ##Attributes for 3rd Convolution Layer\n    conv_attributes[2][\"in_channels\"]=conv_attributes[1][\"out_channels\"]\n    conv_attributes[2][\"out_channels\"]=config.conv_attributes_channels[2]\n    conv_attributes[2][\"kernel_size\"]=config.conv_attributes_kernel_size[2]\n\n    ##Attributes for 4th Convolution Layer\n    conv_attributes[3][\"in_channels\"]=conv_attributes[2][\"out_channels\"]\n    conv_attributes[3][\"out_channels\"]=config.conv_attributes_channels[3]\n    conv_attributes[3][\"kernel_size\"]=config.conv_attributes_kernel_size[3]\n\n    ##Attributes for 5th Convolution Layer\n    conv_attributes[4][\"in_channels\"]=conv_attributes[3][\"out_channels\"]\n    conv_attributes[4][\"out_channels\"]=config.conv_attributes_channels[4]\n    conv_attributes[4][\"kernel_size\"]=config.conv_attributes_kernel_size[4]\n\n    pool_attributes = [{\"kernel_size\":1, \"stride\": 1},\n                     {\"kernel_size\":1, \"stride\": 1},\n                     {\"kernel_size\":1, \"stride\": 1},\n                     {\"kernel_size\":1, \"stride\": 1},\n                     {\"kernel_size\":1, \"stride\": 1}]\n\n    ##Attributes for 1st Pooling Layer\n    pool_attributes[0][\"kernel_size\"]=config.pool_attributes_kernel_size[0]\n    pool_attributes[0][\"stride\"]=config.pool_attributes_stride[0]\n\n    ##Attributes for 2nd Pooling Layer\n    pool_attributes[1][\"kernel_size\"]=config.pool_attributes_kernel_size[1]\n    pool_attributes[1][\"stride\"]=config.pool_attributes_stride[1]\n  \n    ##Attributes for 3rd Pooling Layer\n    pool_attributes[2][\"kernel_size\"]=config.pool_attributes_kernel_size[2]\n    pool_attributes[2][\"stride\"]=config.pool_attributes_stride[2]\n\n    ##Attributes for 4th Pooling Layer\n    pool_attributes[3][\"kernel_size\"]=config.pool_attributes_kernel_size[3]\n    pool_attributes[3][\"stride\"]=config.pool_attributes_stride[3]\n\n    ##Attributes for 5th Pooling Layer\n    pool_attributes[4][\"kernel_size\"]=config.pool_attributes_kernel_size[4]\n    pool_attributes[4][\"stride\"]=config.pool_attributes_stride[4]\n\n    ##Calculating the input dimension for the Dense Linear layer\n    final_dim=LinearInFeatureCalculate(resized_shape,conv_attributes,pool_attributes) #height,width of the dense layer\n    in_feature = (final_dim ** 2) * conv_attributes[4][\"out_channels\"] #number of input nodes in the dense layer  \n \n    #If the enable_GPU flag is on then the run will use GPU\n    if enable_GPU == 1:\n        model = CnnModel(conv_attributes, pool_attributes,in_feature,activation_name,batch_normalization,dropout,dense_layer_size).to(Device)\n    else :\n        model = CnnModel(conv_attributes, pool_attributes,in_feature,activation_name,batch_normalization,dropout,dense_layer_size)\n\n    #Function for training the model with parameters model,num_epochs, batch_size,learning_rate,optimizer_name\n    TrainNetwork(model,num_epochs, batch_size,learning_rate,optimizer_name,weight_decay,resized_shape,actual_data_path,dataset_augmentation,True)\n  \n    #Deleting the model after use\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()\n  ","metadata":{"id":"LzOiIdYQyM7S","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Run this cell to start sweep\nwandb.agent(sweep_id, train_wandb , project=\"Assignment2_PartA\",count=1)\n\nwandb.finish()","metadata":{"id":"CTJeBPEF6XVb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Main function**","metadata":{"id":"KS9k_0LHD9Y6"}},{"cell_type":"code","source":"#Main function to build train and test the model without sweep\ndef main():\n    print(\"Hello\")\n    resized_shape = 256\n\n    ##Hyper-parameters of the model training like number of epochs, batch size, learning rate\n    num_epochs=2\n    batch_size=64\n    learning_rate=0.001\n    optimizer_name = \"Adam\"\n    weight_decay=0.0\n    #For data augmentation\n    dataset_augmentation = True\n\n    actual_data_path = \"./inaturalist_12K\"\n    conv_attributes = [{\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1},\n                     {\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1},\n                     {\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1},\n                     {\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1},\n                     {\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1}]\n\n\n    ##Attributes for 1st Convolution Layer\n    conv_attributes[0][\"in_channels\"]=3\n    conv_attributes[0][\"out_channels\"]=6\n    conv_attributes[0][\"kernel_size\"]=11\n\n    ##Attributes for 2nd Convolution Layer\n    conv_attributes[1][\"in_channels\"]=conv_attributes[0][\"out_channels\"]\n    conv_attributes[1][\"out_channels\"]=12\n    conv_attributes[1][\"kernel_size\"]=9\n\n    ##Attributes for 3rd Convolution Layer\n    conv_attributes[2][\"in_channels\"]=conv_attributes[1][\"out_channels\"]\n    conv_attributes[2][\"out_channels\"]=16\n    conv_attributes[2][\"kernel_size\"]=7\n\n    ##Attributes for 4th Convolution Layer\n    conv_attributes[3][\"in_channels\"]=conv_attributes[2][\"out_channels\"]\n    conv_attributes[3][\"out_channels\"]=32\n    conv_attributes[3][\"kernel_size\"]=5\n\n    ##Attributes for 5th Convolution Layer\n    conv_attributes[4][\"in_channels\"]=conv_attributes[3][\"out_channels\"]\n    conv_attributes[4][\"out_channels\"]=32\n    conv_attributes[4][\"kernel_size\"]=3\n\n    pool_attributes = [{\"kernel_size\":1, \"stride\": 1},\n                     {\"kernel_size\":1, \"stride\": 1},\n                     {\"kernel_size\":1, \"stride\": 1},\n                     {\"kernel_size\":1, \"stride\": 1},\n                     {\"kernel_size\":1, \"stride\": 1}]\n\n    ##Attributes for 1st Pooling Layer\n    pool_attributes[0][\"kernel_size\"]=3\n    pool_attributes[0][\"stride\"]=2\n\n    ##Attributes for 2nd Pooling Layer\n    pool_attributes[1][\"kernel_size\"]=3\n    pool_attributes[1][\"stride\"]=2\n\n    ##Attributes for 3rd Pooling Layer\n    pool_attributes[2][\"kernel_size\"]=3\n    pool_attributes[2][\"stride\"]=2\n\n    ##Attributes for 4th Pooling Layer\n    pool_attributes[3][\"kernel_size\"]=2\n    pool_attributes[3][\"stride\"]=2\n\n    ##Attributes for 5th Pooling Layer\n    pool_attributes[4][\"kernel_size\"]=2\n    pool_attributes[4][\"stride\"]=2\n\n    ##Calculating the input dimension for the Dense Linear layer\n    final_dim=LinearInFeatureCalculate(resized_shape,conv_attributes,pool_attributes) #height,width of the dense layer\n    in_feature = (final_dim ** 2) * conv_attributes[4][\"out_channels\"] #number of input nodes in the dense layer\n\n    #Select the activation function\n    activation_name = 'relu'\n    #Batch Normalization\n    batch_normalization = True\n    #Dropout used\n    dropout = 0\n    #dense layer size\n    dense_layer_size = 32\n    #If the enable_GPU flag is on then the run will use GPU\n    if enable_GPU == 1:\n        model = CnnModel(conv_attributes, pool_attributes,in_feature,activation_name,batch_normalization,dropout,dense_layer_size).to(Device)\n    else :\n        model = CnnModel(conv_attributes, pool_attributes,in_feature,activation_name,batch_normalization,dropout,dense_layer_size)\n    #Function for training the model with parameters model,num_epochs, batch_size,learning_rate,optimizer_name\n    TrainNetwork(model,num_epochs, batch_size,learning_rate,optimizer_name,weight_decay,resized_shape,actual_data_path,dataset_augmentation,False)\n\n    #Function for testing the model accuracy on the test data with parameters model,num_epochs, batch_size,learning_rate\n    TestNetwork(model,num_epochs, batch_size,learning_rate,resized_shape,actual_data_path)","metadata":{"id":"pejeGIEMD87w","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if  __name__ ==\"__main__\":\n    main()","metadata":{"id":"dxEPk_H5FT4_","outputId":"b463363b-7d20-499d-ce31-3f230a56e344","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Guided Backpropagation**","metadata":{}},{"cell_type":"code","source":"#Class for performing guided backprop\n#overrides and clamps gradients using hook registration in pytorch\nclass gb_backprop():\n    def __init__(self, network):\n        self.prev_acts = []\n        self.network = network\n        self.network.eval()\n        self.hooks()\n        self.rec = None\n\n    def hooks(self):\n\n        def full_hook_f(component, input, output):\n            self.prev_acts.append(output)\n\n        def full_hook_b(component, in_der, out_der):\n            derivative = self.prev_acts.pop() \n            derivative[derivative > 0] = 1 \n            \n            positive_out_der = torch.clamp(out_der[0], min=0.0)\n            new_in_der = positive_out_der * derivative\n\n            return (new_in_der,)\n\n        def conv_hook(component, in_der, out_der):\n            self.rec = in_der[0]\n\n        components = list(self.network.named_children())\n\n        for index, component in components:\n            if isinstance(component, nn.ReLU):\n                component.register_forward_hook(full_hook_f)\n                component.register_backward_hook(full_hook_b)\n\n        f_layer = components[0][1]\n        f_layer.register_backward_hook(conv_hook)\n\n    def gb_output(self, image, label):\n        network_output = self.network(image)\n        self.network.zero_grad()\n        mode_pred = network_output.argmax().item()\n        \n        gr_label = torch.zeros(network_output.shape,\n                                      dtype=torch.float)\n        if label is not None:\n            gr_label[0][label] = 1\n        else:\n            gr_label[0][mode_pred] = 1\n        \n        network_output.backward(gr_label)\n        final = self.rec.data[0].permute(1,2,0)\nreturn final.numpy()\n","metadata":{"id":"BwRSrf-_hDeW","_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def grad_nor(grad):\n    modify = (grad-grad.mean())/grad.std()\n    modify = modify * 0.1\n    modify = modify + 0.5\n    modify = modify.clip(0, 1)\n    return modify","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generating_plots(mynet,batch_size):\n    resized_shape=256\n    test_transforms = A.Compose([A.Resize(resized_shape,resized_shape),A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),ToTensorV2()])\n\n    #Function to create test dataset and returns the test image paths\n    test_image_paths=CreateTestDataset(actual_data_path)\n\n    #Test Dataset created\n    test_dataset = iNaturalist_12KDataset(test_image_paths,test_transforms)\n\n    #Dataloader loads test dataset\n    test_loader = DataLoader(\n    test_dataset, batch_size=batch_size, shuffle=True)\n    guided_bp = gb_backprop(mynet.cpu())\n\n    fig = plt.figure(figsize=(9,30))\n    k = 1\n    f = k + 1\n    for images, labels in test_loader:\n        ip = images\n        labels = labels\n        break\n    print(ip.shape)\n    print(labels.shape)\n    print(\"HELLO 4----------\")\n    for i in range(10):\n        tensor = ip[i].unsqueeze(0).requires_grad_()\n        result = guided_bp.gb_output(tensor, labels[i])\n        result = grad_nor(result)\n\n        ax = fig.add_subplot(10,2,k)\n        plt.imshow(de_normalize(ip[i]))\n        plt.xticks([])\n        plt.yticks([])\n        ax = fig.add_subplot(10,2,f)\n        plt.imshow(result)\n\n        k = k + 2\n        f = k + 1\n        plt.xticks([])\n        plt.yticks([])\n        fig.tight_layout()\n    fig.savefig(\"GuidedBackprop\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}