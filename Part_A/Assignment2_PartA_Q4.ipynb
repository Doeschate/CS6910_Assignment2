{"cells":[{"cell_type":"markdown","metadata":{"id":"i9oMWDx10UUx"},"source":["##ASSIGNMENT-2"]},{"cell_type":"markdown","metadata":{"id":"CQnccrq2ewXw"},"source":["Learn how to use CNNs: train from scratch, finetune a pretrained model, use a pre-trained model as it is.\n"]},{"cell_type":"markdown","metadata":{"id":"uifpKNB1odVq"},"source":["**Installs**"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:46:47.156531Z","iopub.status.busy":"2022-04-03T15:46:47.155888Z","iopub.status.idle":"2022-04-03T15:47:14.583509Z","shell.execute_reply":"2022-04-03T15:47:14.582695Z","shell.execute_reply.started":"2022-04-03T15:46:47.156430Z"},"id":"EAGlgBb8okJh","outputId":"791f2ce5-62d4-42dd-f29a-eec043a5479d","trusted":true},"outputs":[],"source":["!pip install -U albumentations\n","!pip install \"opencv-python-headless<4.3\" #for import albumentations as A\n","!pip install wandb #To install wandb and evaluate models"]},{"cell_type":"markdown","metadata":{"id":"Vfgl_cijfI9g"},"source":["**Imports**"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:47:14.587067Z","iopub.status.busy":"2022-04-03T15:47:14.586852Z","iopub.status.idle":"2022-04-03T15:47:18.311927Z","shell.execute_reply":"2022-04-03T15:47:18.311096Z","shell.execute_reply.started":"2022-04-03T15:47:14.587042Z"},"id":"qpDHokXY8ffl","trusted":true},"outputs":[],"source":["import os\n","import copy\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import models,datasets,transforms\n","import torchvision\n","\n","from torch.utils.data import Dataset, DataLoader, ConcatDataset\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","import cv2\n","import glob\n","import numpy as np\n","import random\n","import wandb\n","import gc\n","%matplotlib inline\n","%config InlineBackend.figure_format = 'retina'\n","\n","import matplotlib.pyplot as plt\n","from itertools import chain\n","enable_GPU = 0"]},{"cell_type":"markdown","metadata":{"id":"cBA2e2NrmwYD"},"source":["**Enabling GPU**"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:47:18.315145Z","iopub.status.busy":"2022-04-03T15:47:18.314867Z","iopub.status.idle":"2022-04-03T15:47:18.370227Z","shell.execute_reply":"2022-04-03T15:47:18.369505Z","shell.execute_reply.started":"2022-04-03T15:47:18.315109Z"},"id":"UajdMgmgn3MR","trusted":true},"outputs":[],"source":["Device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(torch.cuda.get_device_name(0))\n","enable_GPU = 1"]},{"cell_type":"markdown","metadata":{"id":"3i4cMnGWFdrG"},"source":["**Download iNaturalist-12K dataset**"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:47:18.371849Z","iopub.status.busy":"2022-04-03T15:47:18.371556Z","iopub.status.idle":"2022-04-03T15:48:05.590597Z","shell.execute_reply":"2022-04-03T15:48:05.589781Z","shell.execute_reply.started":"2022-04-03T15:47:18.371792Z"},"trusted":true},"outputs":[],"source":["!wget https://storage.googleapis.com/wandb_datasets/nature_12K.zip\n","!unzip nature_12K.zip"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:48:05.592335Z","iopub.status.busy":"2022-04-03T15:48:05.592105Z","iopub.status.idle":"2022-04-03T15:48:06.582178Z","shell.execute_reply":"2022-04-03T15:48:06.581256Z","shell.execute_reply.started":"2022-04-03T15:48:05.592308Z"},"trusted":true},"outputs":[],"source":["!rm -r nature_12K.zip\n","actual_data_path = \"./inaturalist_12K\""]},{"cell_type":"markdown","metadata":{"id":"DgxaBE_29k3U"},"source":["**Dataset Creating**"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:48:06.584896Z","iopub.status.busy":"2022-04-03T15:48:06.584625Z","iopub.status.idle":"2022-04-03T15:48:06.729035Z","shell.execute_reply":"2022-04-03T15:48:06.728260Z","shell.execute_reply.started":"2022-04-03T15:48:06.584859Z"},"id":"ora_zBPn7o1i","trusted":true},"outputs":[],"source":["# get all the paths from train_data_path and returns image paths for train and validation set\n","def CreateTrainDataset(actual_data_path):\n","    train_data_path = os.path.join(actual_data_path, \"train\")\n","    train_image_paths = [] #to store image paths in list\n","    classes = [] #to store class values\n","    for data_path in glob.glob(train_data_path + \"/*\"):\n","        train_image_paths.append(glob.glob(data_path + '/*')) #stores all the training image paths in this list\n","    train_image_paths = list(chain.from_iterable(train_image_paths))\n","    random.shuffle(train_image_paths)\n","\n","    # split train valid from train paths (90,10)\n","    train_image_paths, valid_image_paths = train_image_paths[:int(0.9*len(train_image_paths))], train_image_paths[int(0.9*len(train_image_paths)):] \n","    return train_image_paths, valid_image_paths\n","\n","# create the test_image_paths\n","def CreateTestDataset(actual_data_path):\n","    test_data_path = os.path.join(actual_data_path, \"val\")\n","    test_image_paths = []\n","    for data_path in glob.glob(test_data_path + '/*'):\n","        test_image_paths.append(glob.glob(data_path + '/*')) #stores all the test images path in this list\n","    test_image_paths = list(chain.from_iterable(test_image_paths))\n","    return test_image_paths"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:48:06.731154Z","iopub.status.busy":"2022-04-03T15:48:06.730845Z","iopub.status.idle":"2022-04-03T15:48:06.742708Z","shell.execute_reply":"2022-04-03T15:48:06.742048Z","shell.execute_reply.started":"2022-04-03T15:48:06.731093Z"},"id":"8CxrpoqISjgI","trusted":true},"outputs":[],"source":["#Create dictionary for class indexes\n","train_data_path = os.path.join(actual_data_path, \"train\")\n","classes = [] #to store class values\n","for data_path in glob.glob(train_data_path + \"/*\"):\n","    classes.append(data_path.split('/')[-1])\n","idx_to_class = {i:j for i, j in enumerate(classes)} #index to class map\n","class_to_idx = {value:key for key,value in idx_to_class.items()} #class to index map"]},{"cell_type":"markdown","metadata":{"id":"-B7uy3GTY1gi"},"source":["**Dataset Class**"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:48:06.745964Z","iopub.status.busy":"2022-04-03T15:48:06.745734Z","iopub.status.idle":"2022-04-03T15:48:06.753395Z","shell.execute_reply":"2022-04-03T15:48:06.752431Z","shell.execute_reply.started":"2022-04-03T15:48:06.745933Z"},"id":"gZs1aTpeVCst","trusted":true},"outputs":[],"source":["#Function returns images and corresponding lebels after performing transforms\n","class iNaturalist_12KDataset(Dataset):\n","    def __init__(self, image_paths, transform=False):\n","        self.image_paths = image_paths\n","        self.transform = transform\n","        \n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        image_filepath = self.image_paths[idx]\n","        image = cv2.imread(image_filepath)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        \n","        label = image_filepath.split('/')[-2]\n","        label = class_to_idx[label]\n","        if self.transform is not None:\n","            image = self.transform(image=image)[\"image\"]\n","        return image, label"]},{"cell_type":"markdown","metadata":{"id":"y53VI2iPpO7k"},"source":["**Building the Model**"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:48:06.755542Z","iopub.status.busy":"2022-04-03T15:48:06.754766Z","iopub.status.idle":"2022-04-03T15:48:06.764370Z","shell.execute_reply":"2022-04-03T15:48:06.763688Z","shell.execute_reply.started":"2022-04-03T15:48:06.755493Z"},"id":"5trpZKH-nIxp","trusted":true},"outputs":[],"source":["#Optimization Function\n","def OptimizerFunction(model, learning_rate,weight_decay, optimizer_name):\n","    if optimizer_name == \"SGD\":\n","        opt = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","    elif optimizer_name == \"Adam\":\n","        opt = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","    del model\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    return opt\n","    "]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:48:06.766346Z","iopub.status.busy":"2022-04-03T15:48:06.765778Z","iopub.status.idle":"2022-04-03T15:48:06.772558Z","shell.execute_reply":"2022-04-03T15:48:06.771879Z","shell.execute_reply.started":"2022-04-03T15:48:06.766311Z"},"id":"kb7hUIJfxQHX","trusted":true},"outputs":[],"source":["#Loss Function\n","def LossFunction():\n","    return nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:48:06.774603Z","iopub.status.busy":"2022-04-03T15:48:06.773753Z","iopub.status.idle":"2022-04-03T15:48:06.781079Z","shell.execute_reply":"2022-04-03T15:48:06.780263Z","shell.execute_reply.started":"2022-04-03T15:48:06.774561Z"},"id":"md4p1DgOJhRH","trusted":true},"outputs":[],"source":["#Activation Function\n","#To add another activation just add another else statement for that activation and return the corresponding pytorch reference for that activation\n","def ActivationFunction(activation_name):\n","    if(activation_name == 'relu'):\n","        return F.relu  \n","    elif(activation_name == 'elu'):\n","        return F.elu\n","    elif(activation_name == 'sigmoid'):\n","        return F.sigmoid\n","    elif(activation_name == 'gelu'):\n","        return F.gelu\n","    else:\n","        return None"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:48:06.786201Z","iopub.status.busy":"2022-04-03T15:48:06.785710Z","iopub.status.idle":"2022-04-03T15:48:06.808365Z","shell.execute_reply":"2022-04-03T15:48:06.807426Z","shell.execute_reply.started":"2022-04-03T15:48:06.786169Z"},"id":"-fhgXbvhiExl","trusted":true},"outputs":[],"source":["class CnnModel(nn.Module):\n","    def __init__(self, conv_attributes, pool_attributes,in_feature,activation_name,batch_normalization,dropout,dense_layer_size):\n","        super(CnnModel, self).__init__()\n","\n","        self.batch_normalization = batch_normalization #batch normalization \n","        self.dropout = dropout # dropout\n","        #First Convolution and Pooling Layer\n","        self.conv1= nn.Conv2d(conv_attributes[0][\"in_channels\"], conv_attributes[0][\"out_channels\"], conv_attributes[0][\"kernel_size\"])\n","        self.bn1 = nn.BatchNorm2d(conv_attributes[0][\"out_channels\"])\n","        self.act1 = ActivationFunction(activation_name)\n","        self.pool1= nn.MaxPool2d(pool_attributes[0][\"kernel_size\"], pool_attributes[0][\"stride\"])\n","\n","        #Second Convolution and Pooling Layer\n","        self.conv2= nn.Conv2d(conv_attributes[1][\"in_channels\"], conv_attributes[1][\"out_channels\"], conv_attributes[1][\"kernel_size\"])\n","        self.bn2 = nn.BatchNorm2d(conv_attributes[1][\"out_channels\"])\n","        self.act2 = ActivationFunction(activation_name)\n","        self.pool2= nn.MaxPool2d(pool_attributes[1][\"kernel_size\"], pool_attributes[1][\"stride\"])\n","\n","        #Third Convolution and Pooling Layer\n","        self.conv3= nn.Conv2d(conv_attributes[2][\"in_channels\"], conv_attributes[2][\"out_channels\"], conv_attributes[2][\"kernel_size\"])\n","        self.bn3 = nn.BatchNorm2d(conv_attributes[2][\"out_channels\"])\n","        self.act3 = ActivationFunction(activation_name)\n","        self.pool3= nn.MaxPool2d(pool_attributes[2][\"kernel_size\"], pool_attributes[2][\"stride\"])\n","\n","        #Fourth Convolution and Pooling Layer\n","        self.conv4= nn.Conv2d(conv_attributes[3][\"in_channels\"], conv_attributes[3][\"out_channels\"], conv_attributes[3][\"kernel_size\"])\n","        self.bn4 = nn.BatchNorm2d(conv_attributes[3][\"out_channels\"])\n","        self.act4 = ActivationFunction(activation_name)\n","        self.pool4= nn.MaxPool2d(pool_attributes[3][\"kernel_size\"], pool_attributes[3][\"stride\"])\n","\n","        #Fifth Convolution and Pooling Layer\n","        self.conv5= nn.Conv2d(conv_attributes[4][\"in_channels\"], conv_attributes[4][\"out_channels\"], conv_attributes[4][\"kernel_size\"])\n","        self.bn5 = nn.BatchNorm2d(conv_attributes[4][\"out_channels\"])\n","        self.act5 = ActivationFunction(activation_name)\n","        self.pool5= nn.MaxPool2d(pool_attributes[4][\"kernel_size\"], pool_attributes[4][\"stride\"])\n","\n","        #First Dense Layer\n","        self.fc1 = nn.Linear(in_feature, dense_layer_size)\n","        self.fc1_act = ActivationFunction(activation_name)\n","        self.fc2 = nn.Linear(dense_layer_size, 10)\n","\n","    def forward(self,x):\n","        if self.batch_normalization:\n","            y = self.conv1(x)\n","            x = self.pool1(self.act1(self.bn1(y))) #First block of layer containing one conv layer with batch normalization and activation function followed by one pooling layer\n","            x = self.pool2(self.act2(self.bn2(self.conv2(x)))) #Second block of layer containing one conv layer with batch normalization and activation function followed by one pooling layer\n","            x = self.pool3(self.act3(self.bn3(self.conv3(x)))) #Third block of layer containing one conv layer with batch normalization and activation function followed by one pooling layer\n","            x = self.pool4(self.act4(self.bn4(self.conv4(x)))) #Fourth block of layer containing one conv layer with batch normalization and activation function followed by one pooling layer\n","            x = self.pool5(self.act5(self.bn5(self.conv5(x)))) #Fifth block of layer containing one conv layer with batch normalization and activation function followed by one pooling layer\n","        else:\n","            x = self.pool1(self.act1(self.conv1(x))) #First block of layer containing one conv layer with  activation function followed by one pooling layer\n","            x = self.pool2(self.act2(self.conv2(x))) #Second block of layer containing one conv layer with  activation function followed by one pooling layer\n","            x = self.pool3(self.act3(self.conv3(x))) #Third block of layer containing one conv layer with  activation function followed by one pooling layer\n","            x = self.pool4(self.act4(self.conv4(x))) #Fourth block of layer containing one conv layer with  activation function followed by one pooling layer\n","            x = self.pool5(self.act5(self.conv5(x))) #Fifth block of layer containing one conv layer with  activation function followed by one pooling layer\n","\n","        x = torch.flatten(x, 1) # flatten all dimensions except batch\n","        x = self.fc1(x)\n","        x = self.fc1_act(x)\n","        x = nn.Dropout(self.dropout)(x)\n","        x = self.fc2(x)\n","        x = F.softmax(x,dim=1)                     \n","        return x"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:48:06.811446Z","iopub.status.busy":"2022-04-03T15:48:06.810990Z","iopub.status.idle":"2022-04-03T15:48:06.832369Z","shell.execute_reply":"2022-04-03T15:48:06.831637Z","shell.execute_reply.started":"2022-04-03T15:48:06.811408Z"},"id":"k8ku7nlNhwlc","trusted":true},"outputs":[],"source":["#Training Function\n","def TrainNetwork(model,num_epochs, batch_size,learning_rate,optimizer_name,weight_decay,resized_shape,actual_data_path,dataset_augmentation,wandb_fn):\n","    loss_funt = LossFunction() #Loss function is called\n","    optimizer = OptimizerFunction(model, learning_rate, weight_decay, optimizer_name) #Optimization function is called\n","    #Calling Compose returns a transform function that performs image transformation.\n","    train_transforms = A.Compose([A.Resize(resized_shape,resized_shape),A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),ToTensorV2()])\n","  \n","    if dataset_augmentation:\n","        augmented_transforms = A.Compose([A.SmallestMaxSize(max_size=350),\n","              A.Resize(resized_shape,resized_shape),\n","              A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=360, p=0.5),\n","              A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n","              A.RandomBrightnessContrast(p=0.5),\n","              A.MultiplicativeNoise(multiplier=[0.5,2], per_channel=True, p=0.2),\n","              A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n","              A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n","              A.RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n","              ToTensorV2()])\n","  \n","    #Function to create train, validation dataset and returns the train and validation image paths\n","    train_image_paths, valid_image_paths=CreateTrainDataset(actual_data_path)\n","    #Training Dataset created with train_transforms\n","    train_dataset = iNaturalist_12KDataset(train_image_paths,train_transforms)\n","    if dataset_augmentation:\n","        transformed_dataset = iNaturalist_12KDataset(train_image_paths,augmented_transforms)   #Transformed Dataset created with augmented_transforms\n","        train_dataset = torch.utils.data.ConcatDataset([transformed_dataset,train_dataset])\n","    #Validation Dataset created\n","    valid_dataset = iNaturalist_12KDataset(valid_image_paths,train_transforms) #train transforms are applied\n","    #Dataloader loads train dataset\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    #Dataloader loads validation dataset\n","    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n","    #Training the network\n","    n_total_steps = len(train_loader)\n","    total_data_size = len(train_loader.dataset)\n","    for epoch in range(num_epochs):\n","        cumulative_loss = 0  \n","        correct_training = 0  \n","        model.train(True) # For training\n","        for i, (images, labels) in enumerate(train_loader):\n","            if enable_GPU == 1 :\n","                images = images.to(Device)\n","                labels = labels.to(Device)\n","\n","            # Forward pass\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            correct_training += (predicted == labels).sum().item()\n","            loss = loss_funt(outputs, labels)\n","\n","            # Backward and optimize\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            cumulative_loss += loss.item()\n","            if (i+1) % 30 == 0:\n","                print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n","\n","        print('Finished Training---------------------')\n","\n","        #Validating the trained model\n","        with torch.no_grad():\n","            n_valid_steps = len(valid_loader)\n","            total_validation_data = len(valid_loader.dataset)\n","            model.train(False)\n","            correct_validation = 0\n","            for i, (images, labels) in enumerate(valid_loader):\n","                if enable_GPU == 1 :\n","                    images = images.to(Device)\n","                    labels = labels.to(Device)\n","                # Forward pass\n","                outputs = model(images)\n","                _, predicted = torch.max(outputs.data, 1)\n","                correct_validation += (predicted == labels).sum().item()\n","        print(\"Train Accuracy in Epoch {0}/{1} = {2} : \".format(epoch+1 , num_epochs , 100*(correct_training/total_data_size))) \n","        print(\"Val Accuracy in Epoch {0}/{1} = {2} : \".format(epoch+1 , num_epochs , 100*(correct_validation/total_validation_data)))  \n","        print(\"Loss in Epoch {0}/{1} = {2} : \".format(epoch+1 , num_epochs , cumulative_loss/total_data_size))  \n","        print()\n","    PlotGridOfImages(model,batch_size)\n","    visualize_filters(model,batch_size)\n","    #Deleting the model after use\n","    del model\n","    gc.collect()\n","    torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:48:06.834380Z","iopub.status.busy":"2022-04-03T15:48:06.833884Z","iopub.status.idle":"2022-04-03T15:48:06.843152Z","shell.execute_reply":"2022-04-03T15:48:06.842458Z","shell.execute_reply.started":"2022-04-03T15:48:06.834342Z"},"id":"lBScrogGDsZv","trusted":true},"outputs":[],"source":["##Calculates the input feature for the dense linear layer\n","def LinearInFeatureCalculate(initial_dim,conv_attributes,pool_attributes):\n","    for i in range(5):\n","        D = (initial_dim + 2*conv_attributes[i][\"padding\"] - conv_attributes[i][\"dilation\"]*(conv_attributes[i][\"kernel_size\"]-1) - 1)//(conv_attributes[i][\"stride\"]) + 1\n","        D = (D - pool_attributes[i][\"kernel_size\"])//(pool_attributes[i][\"stride\"]) + 1\n","        initial_dim = D\n","    return D\n"]},{"cell_type":"code","execution_count":15,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-04-03T15:48:06.845606Z","iopub.status.busy":"2022-04-03T15:48:06.844029Z","iopub.status.idle":"2022-04-03T15:48:06.852215Z","shell.execute_reply":"2022-04-03T15:48:06.851549Z","shell.execute_reply.started":"2022-04-03T15:48:06.845567Z"},"trusted":true},"outputs":[],"source":["#Supporting function for plotting visualizations\n","def de_normalize(img):\n","    mean, std = (0.485, 0.456, 0.406),(0.229, 0.224, 0.225)\n","    z = img * torch.tensor(std).view(3, 1, 1)\n","    z = z + torch.tensor(mean).view(3, 1, 1)\n","\n","    img2 = transforms.ToPILImage(mode='RGB')(z)\n","    return img2"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:48:06.855602Z","iopub.status.busy":"2022-04-03T15:48:06.853150Z","iopub.status.idle":"2022-04-03T15:48:06.867407Z","shell.execute_reply":"2022-04-03T15:48:06.866702Z","shell.execute_reply.started":"2022-04-03T15:48:06.855564Z"},"trusted":true},"outputs":[],"source":["#Visualizing filters of Conv1 layer\n","def visualize_filters(model,batch_size):\n","    resized_shape=256\n","    test_transforms = A.Compose([A.Resize(resized_shape,resized_shape),A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),ToTensorV2()])\n","\n","    #Function to create test dataset and returns the test image paths\n","    test_image_paths=CreateTestDataset(actual_data_path)\n","\n","    #Test Dataset created\n","    test_dataset = iNaturalist_12KDataset(test_image_paths,test_transforms)\n","\n","    #Dataloader loads test dataset\n","    test_loader = DataLoader(\n","    test_dataset, batch_size=batch_size, shuffle=True)\n","    \n","    \n","    \n","    for images, labels in test_loader:\n","        img = images\n","    \n","    w = model.cpu().conv1.weight.data\n","    im_test = model.cpu().conv1(img)\n","    im_test = im_test[0]\n","    im_test= im_test[None, :]\n","    im_test=im_test.permute(1,0,2,3)\n","    del model\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    grid_w = torchvision.utils.make_grid(w, nrow=16, normalize=True, scale_each=True, )\n","    grid_im = torchvision.utils.make_grid(im_test, nrow=16, normalize=True, scale_each=True, )\n","\n","    plt.figure(figsize=(30, 30))\n","    plt.title(\"Visualizationg of Filters of Conv1\")\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.imshow(grid_w.permute(1, 2, 0))\n","    plt.savefig(\"filtersconv.png\")\n","    \n","    \n","    plt.figure(figsize=(30, 30))\n","    plt.title(\"Visualizationg of image after using conv1 filters\")\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.imshow(grid_im.permute(1, 2, 0))\n","    plt.savefig(\"filtersimg_test.png\")\n","    "]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:48:06.869350Z","iopub.status.busy":"2022-04-03T15:48:06.868687Z","iopub.status.idle":"2022-04-03T15:48:06.880861Z","shell.execute_reply":"2022-04-03T15:48:06.879983Z","shell.execute_reply.started":"2022-04-03T15:48:06.869313Z"},"trusted":true},"outputs":[],"source":["#Plotting visualizations of 30 predictions\n","def PlotGridOfImages(model,batch_size):\n","    #Function for image augmentation.Calling Compose returns a transform function that performs image augmentation.\n","    batch_size=32\n","    resized_shape=256\n","    print(batch_size)\n","    test_transforms = A.Compose([A.Resize(resized_shape,resized_shape),A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),ToTensorV2()])\n","\n","    #Function to create test dataset and returns the test image paths\n","    test_image_paths=CreateTestDataset(actual_data_path)\n","\n","    #Test Dataset created\n","    test_dataset = iNaturalist_12KDataset(test_image_paths,test_transforms)\n","\n","    #Dataloader loads test dataset\n","    test_loader = DataLoader(\n","    test_dataset, batch_size=batch_size, shuffle=True)\n","    \n","    for images, labels in test_loader:\n","        if enable_GPU == 1:\n","            images = images.to(Device)\n","            labels = labels.to(Device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs, 1)\n","        break\n","    del model\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    #Plotting model predictions\n","    fig = plt.figure(figsize=(10,30))\n","    for x in range(30):\n","        ax = fig.add_subplot(10,3,x+1)\n","        plt.imshow(de_normalize(images[x].cpu()))\n","        plt.xlabel(\"True: \"+ idx_to_class[(labels[x].cpu()).item()])\n","        plt.ylabel(\"Pred: \"+ idx_to_class[(predicted[x].cpu()).item()])\n","        plt.xticks([])\n","        plt.yticks([])\n","    fig.tight_layout()\n","    fig.savefig(\"Plots.png\")\n","    "]},{"cell_type":"markdown","metadata":{"id":"fCPdk3H8AqjZ"},"source":["**Sweep Config**"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:48:06.882690Z","iopub.status.busy":"2022-04-03T15:48:06.882422Z","iopub.status.idle":"2022-04-03T15:49:19.857911Z","shell.execute_reply":"2022-04-03T15:49:19.856496Z","shell.execute_reply.started":"2022-04-03T15:48:06.882654Z"},"trusted":true},"outputs":[],"source":["#This is the config file for performing sweep in wandb\n","sweep_config = {\n","  'name': 'Assignment2_PartA_Q2',\n","  'method': 'bayes',\n","  'metric': {\n","      'name': 'Validation Accuracy',\n","      'goal': 'maximize'   \n","    },\n","  'parameters': {\n","      'epochs': {\n","            'values': [30]\n","        },\n","        'conv_attributes_channels': {\n","            'values': [[256,128,64,32,16]]\n","        },\n","        'conv_attributes_kernel_size': {\n","            'values': [[3,3,5,7,9]]\n","        },\n","        'pool_attributes_kernel_size': {\n","            'values': [[3,3,3,2,2]]\n","        },\n","        'pool_attributes_stride': {\n","            'values': [[2,2,2,2,1]]\n","        },\n","        'dense_layer_size': {\n","            'values': [128]\n","        },\n","        'learning_rate': {\n","            'values': [0.0001]\n","        },\n","        'activation': {\n","            'values': ['gelu']\n","        },\n","        'dropout': {\n","            'values': [0.3]\n","        },\n","        'batch_normalization': {\n","            'values': [True]\n","        },\n","        'batch_size': {\n","            'values': [16]\n","        },\n","        'weight_decay': {\n","            'values': [0.00001]\n","        },\n","        'dataset_augmentation':{\n","              'values': [True]\n","        },\n","        'optimizer_name':{\n","              'values': ['Adam']\n","        }\n","    }\n","}\n","sweep_id = wandb.sweep(sweep_config,entity=\"cs21s045_cs21s011\",project=\"Assignment2_PartA\")"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:49:19.859804Z","iopub.status.busy":"2022-04-03T15:49:19.859546Z","iopub.status.idle":"2022-04-03T15:49:19.881190Z","shell.execute_reply":"2022-04-03T15:49:19.880373Z","shell.execute_reply.started":"2022-04-03T15:49:19.859768Z"},"id":"LzOiIdYQyM7S","trusted":true},"outputs":[],"source":["#This function needs to be passed to sweep_agent\n","def train_wandb():\n","    run = wandb.init()\n","    config = run.config\n","    resized_shape = 256\n","    ##Hyper-parameters of the model training like number of epochs, batch size, learning rate etc from sweep\n","    num_epochs=config.epochs\n","    batch_size=config.batch_size\n","    learning_rate=config.learning_rate\n","    optimizer_name = config.optimizer_name\n","    weight_decay=config.weight_decay\n","    #Select the activation function\n","    activation_name = config.activation\n","    #Batch Normalization\n","    batch_normalization = config.batch_normalization\n","    #Dropout used\n","    dropout = config.dropout\n","    #dense layer size\n","    dense_layer_size = config.dense_layer_size\n","    #For data augmentation\n","    dataset_augmentation = config.dataset_augmentation\n","    conv_attributes = [{\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1},\n","                     {\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1},\n","                     {\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1},\n","                     {\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1},\n","                     {\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1}]\n","  \n","  \n","    ##Attributes for 1st Convolution Layer\n","    conv_attributes[0][\"in_channels\"]=3\n","    conv_attributes[0][\"out_channels\"]=config.conv_attributes_channels[0]\n","    conv_attributes[0][\"kernel_size\"]=config.conv_attributes_kernel_size[0]\n","\n","    ##Attributes for 2nd Convolution Layer\n","    conv_attributes[1][\"in_channels\"]=conv_attributes[0][\"out_channels\"]\n","    conv_attributes[1][\"out_channels\"]=config.conv_attributes_channels[1]\n","    conv_attributes[1][\"kernel_size\"]=config.conv_attributes_kernel_size[1]\n","\n","    ##Attributes for 3rd Convolution Layer\n","    conv_attributes[2][\"in_channels\"]=conv_attributes[1][\"out_channels\"]\n","    conv_attributes[2][\"out_channels\"]=config.conv_attributes_channels[2]\n","    conv_attributes[2][\"kernel_size\"]=config.conv_attributes_kernel_size[2]\n","\n","    ##Attributes for 4th Convolution Layer\n","    conv_attributes[3][\"in_channels\"]=conv_attributes[2][\"out_channels\"]\n","    conv_attributes[3][\"out_channels\"]=config.conv_attributes_channels[3]\n","    conv_attributes[3][\"kernel_size\"]=config.conv_attributes_kernel_size[3]\n","\n","    ##Attributes for 5th Convolution Layer\n","    conv_attributes[4][\"in_channels\"]=conv_attributes[3][\"out_channels\"]\n","    conv_attributes[4][\"out_channels\"]=config.conv_attributes_channels[4]\n","    conv_attributes[4][\"kernel_size\"]=config.conv_attributes_kernel_size[4]\n","\n","    pool_attributes = [{\"kernel_size\":1, \"stride\": 1},\n","                     {\"kernel_size\":1, \"stride\": 1},\n","                     {\"kernel_size\":1, \"stride\": 1},\n","                     {\"kernel_size\":1, \"stride\": 1},\n","                     {\"kernel_size\":1, \"stride\": 1}]\n","\n","    ##Attributes for 1st Pooling Layer\n","    pool_attributes[0][\"kernel_size\"]=config.pool_attributes_kernel_size[0]\n","    pool_attributes[0][\"stride\"]=config.pool_attributes_stride[0]\n","\n","    ##Attributes for 2nd Pooling Layer\n","    pool_attributes[1][\"kernel_size\"]=config.pool_attributes_kernel_size[1]\n","    pool_attributes[1][\"stride\"]=config.pool_attributes_stride[1]\n","  \n","    ##Attributes for 3rd Pooling Layer\n","    pool_attributes[2][\"kernel_size\"]=config.pool_attributes_kernel_size[2]\n","    pool_attributes[2][\"stride\"]=config.pool_attributes_stride[2]\n","\n","    ##Attributes for 4th Pooling Layer\n","    pool_attributes[3][\"kernel_size\"]=config.pool_attributes_kernel_size[3]\n","    pool_attributes[3][\"stride\"]=config.pool_attributes_stride[3]\n","\n","    ##Attributes for 5th Pooling Layer\n","    pool_attributes[4][\"kernel_size\"]=config.pool_attributes_kernel_size[4]\n","    pool_attributes[4][\"stride\"]=config.pool_attributes_stride[4]\n","\n","    ##Calculating the input dimension for the Dense Linear layer\n","    final_dim=LinearInFeatureCalculate(resized_shape,conv_attributes,pool_attributes) #height,width of the dense layer\n","    in_feature = (final_dim ** 2) * conv_attributes[4][\"out_channels\"] #number of input nodes in the dense layer  \n"," \n","    #If the enable_GPU flag is on then the run will use GPU\n","    if enable_GPU == 1:\n","        model = CnnModel(conv_attributes, pool_attributes,in_feature,activation_name,batch_normalization,dropout,dense_layer_size).to(Device)\n","    else :\n","        model = CnnModel(conv_attributes, pool_attributes,in_feature,activation_name,batch_normalization,dropout,dense_layer_size)\n","\n","    #Function for training the model with parameters model,num_epochs, batch_size,learning_rate,optimizer_name\n","    TrainNetwork(model,num_epochs, batch_size,learning_rate,optimizer_name,weight_decay,resized_shape,actual_data_path,dataset_augmentation,True)\n","  \n","    #Deleting the model after use\n","    del model\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-03T15:49:19.884262Z","iopub.status.busy":"2022-04-03T15:49:19.883998Z"},"id":"CTJeBPEF6XVb","trusted":true},"outputs":[],"source":["#Run this cell to start sweep\n","wandb.agent(sweep_id, train_wandb , project=\"Assignment2_PartA\",count=1)\n","\n","wandb.finish()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
