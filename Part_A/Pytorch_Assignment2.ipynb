{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch_Assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jStq0xoQ4BWn",
        "Tkk4w8VZ4T-0",
        "ovjKy6vrJerT",
        "7CANEN4V89A4"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Installing Pytorch##"
      ],
      "metadata": {
        "id": "jStq0xoQ4BWn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsL-n959jUnJ"
      },
      "outputs": [],
      "source": [
        "!pip3 install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing##"
      ],
      "metadata": {
        "id": "Tkk4w8VZ4T-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ZBvgTO4l4KOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "id": "IXq9iluy4Xi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lst = [1,2,3,4]"
      ],
      "metadata": {
        "id": "Wm06_gBR4jEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lst"
      ],
      "metadata": {
        "id": "uLUiroj27IMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr = np.array(lst)"
      ],
      "metadata": {
        "id": "vwZPslyl7JNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr"
      ],
      "metadata": {
        "id": "pt3Bq8T47XXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr.dtype"
      ],
      "metadata": {
        "id": "ieKuFfmY7Ybn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convert numpy to tensor**"
      ],
      "metadata": {
        "id": "gTQ3EMb07nlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensors = torch.from_numpy(arr)"
      ],
      "metadata": {
        "id": "uyXvsK2Y7eZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensors[-1:3]"
      ],
      "metadata": {
        "id": "N9oG-ZLx9__9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_arr = torch.tensor(arr)"
      ],
      "metadata": {
        "id": "f_4W2Z7R-CPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_arr"
      ],
      "metadata": {
        "id": "LTbxbnIJ_BBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_arr[2]=30\n",
        "print(tensor_arr)\n",
        "print(arr)"
      ],
      "metadata": {
        "id": "oWi9c8fb_CSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zeros=torch.zeros(2,3,dtype=torch.int64)\n",
        "ones=torch.ones(2,3,dtype=torch.int64)\n",
        "print(zeros)\n",
        "print(ones)"
      ],
      "metadata": {
        "id": "vaQm54DD_Qut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([2,3,4], dtype=torch.float)\n",
        "b = torch.tensor([5,6,7], dtype=torch.float)"
      ],
      "metadata": {
        "id": "omq-AN0t_2Hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a+b"
      ],
      "metadata": {
        "id": "umUG4PhfE613"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.add(a,b)"
      ],
      "metadata": {
        "id": "Peniyp0XE7-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c=torch.zeros(3)"
      ],
      "metadata": {
        "id": "jvLXxvtwFAjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.add(a,b,out=c)"
      ],
      "metadata": {
        "id": "UGSCMCSKFTx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c"
      ],
      "metadata": {
        "id": "STvn57MrFbIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c.sum()"
      ],
      "metadata": {
        "id": "L9N2XmljFcjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k=a.mul(b)\n",
        "l=a.dot(b)\n",
        "print(k)\n",
        "print(l)"
      ],
      "metadata": {
        "id": "E5BzdcrUFuGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9YhNQkxMGHi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Back propagation using Pytorch"
      ],
      "metadata": {
        "id": "ovjKy6vrJerT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "uDxtZwaOJmp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(5,requires_grad=True, dtype = torch.float)\n",
        "y=x**2\n",
        "y.backward()"
      ],
      "metadata": {
        "id": "-Q-kqJepJpiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "YZ-SZg0ZJ1-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad"
      ],
      "metadata": {
        "id": "7TObgbWhNlLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lst1 = [[1,2,3],[4,5,6],[7,8,9]]\n",
        "tensor_input = torch.tensor(lst1,dtype=torch.float,requires_grad=True)"
      ],
      "metadata": {
        "id": "ynvQi86BNshk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_input"
      ],
      "metadata": {
        "id": "BMRmCb5jckaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=tensor_input**2 + tensor_input**3 "
      ],
      "metadata": {
        "id": "yVMKSKRRcmxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "W1AMHStjc5oH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z= y.sum()"
      ],
      "metadata": {
        "id": "z6WTfvWFc6pH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z"
      ],
      "metadata": {
        "id": "164t-yO5dASa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z.backward()"
      ],
      "metadata": {
        "id": "M4Q1-boUdA9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_input.grad"
      ],
      "metadata": {
        "id": "cEZsD86OdSZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "7CqAb8xQdbp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ANN in Pytorch"
      ],
      "metadata": {
        "id": "7CANEN4V89A4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Kaggle Dataset directly**"
      ],
      "metadata": {
        "id": "lwX9do0IANpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "id": "y8Pq73NH9ARt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "995p45iiAVwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "ZiK3oLuKAkAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "8SOfdW0dAv4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download uciml/pima-indians-diabetes-database"
      ],
      "metadata": {
        "id": "XWsa3mdRBS1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip pima-indians-diabetes-database"
      ],
      "metadata": {
        "id": "9D3DBXFACJs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code**"
      ],
      "metadata": {
        "id": "bqcNd-vpDzxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "ADJe3EOeDy3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('diabetes.csv')"
      ],
      "metadata": {
        "id": "bkOR4t0rCYBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "ESIoQeJSEADo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "u-0hPaEmEBvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=df.drop('Outcome',axis=1).values\n",
        "y=df['Outcome'].values"
      ],
      "metadata": {
        "id": "ydn4jN_nJSCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)"
      ],
      "metadata": {
        "id": "UkfLYYbiMGPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pytorch Model**"
      ],
      "metadata": {
        "id": "Yu7984FdPKfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "D0Y6WWruNBU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = torch.tensor(X_train, dtype = torch.float)\n",
        "X_test = torch.tensor(X_test, dtype = torch.float)"
      ],
      "metadata": {
        "id": "LwAzq-gdQBfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = torch.LongTensor(y_train)\n",
        "y_test = torch.LongTensor(y_test)"
      ],
      "metadata": {
        "id": "dbz18Bk0Qc9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ANN_Model(nn.Module):\n",
        "  def __init__(self,input_features=8,hidden1=20,hidden2=20,out_features=2):\n",
        "    super().__init__()\n",
        "    self.f_connected1 = nn.Linear(input_features,hidden1)\n",
        "    self.f_connected2 = nn.Linear(hidden1,hidden2)\n",
        "    self.out = nn.Linear(hidden2,out_features)\n",
        "  def forward(self,x):\n",
        "    x=F.relu(self.f_connected1(x))\n",
        "    x=F.relu(self.f_connected2(x))\n",
        "    x=self.out(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "BrtaAmsOQ6kE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(20)\n",
        "model=ANN_Model()"
      ],
      "metadata": {
        "id": "ZDacCEnCam4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.parameters"
      ],
      "metadata": {
        "id": "HO4uRKf9atWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function=nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.01)"
      ],
      "metadata": {
        "id": "qvf6Y_K9ayNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=500\n",
        "final_loss=[]\n",
        "for i in range(epochs):\n",
        "  i=i+1\n",
        "  y_pred=model.forward(X_train)\n",
        "  loss=loss_function(y_pred,y_train)\n",
        "  final_loss.append(loss)\n",
        "  if i%10==0:\n",
        "    print(\"Epoch number: {} and the loss: {}\".format(i,loss.item()))\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n"
      ],
      "metadata": {
        "id": "IoYNBNbYcFZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions=[]\n",
        "with torch.no_grad():\n",
        "  for i,data in enumerate(X_test):\n",
        "    y_pred=model(data)\n",
        "    predictions.append(y_pred.argmax().item())\n",
        "    # print(y_pred,y_pred.argmax().item())"
      ],
      "metadata": {
        "id": "teHR92MAeBgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test,predictions)\n",
        "cm"
      ],
      "metadata": {
        "id": "blmTuaCWhsRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "score=accuracy_score(y_test,predictions)"
      ],
      "metadata": {
        "id": "osPvoWIJih6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score"
      ],
      "metadata": {
        "id": "8Z0lVyvKjONS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VQfn2KC9jPTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ASSIGNMENT-2"
      ],
      "metadata": {
        "id": "i9oMWDx10UUx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learn how to use CNNs: train from scratch, finetune a pretrained model, use a pre-trained model as it is.\n"
      ],
      "metadata": {
        "id": "CQnccrq2ewXw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Imports"
      ],
      "metadata": {
        "id": "Vfgl_cijfI9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U albumentations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpDHokXY8ffl",
        "outputId": "e41e0a80-d7c5-4e36-8393-fe44064f4268"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.7/dist-packages (0.1.12)\n",
            "Collecting albumentations\n",
            "  Downloading albumentations-1.1.0-py3-none-any.whl (102 kB)\n",
            "\u001b[K     |████████████████████████████████| 102 kB 17.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.21.5)\n",
            "Collecting opencv-python-headless>=4.1.1\n",
            "  Downloading opencv_python_headless-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.8 MB 2.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.4.1)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (0.18.3)\n",
            "Collecting qudida>=0.0.4\n",
            "  Downloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations) (3.13)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations) (3.10.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations) (1.0.2)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (3.2.2)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2021.11.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.4.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (7.1.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.6.3)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (3.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.1.0)\n",
            "Installing collected packages: opencv-python-headless, qudida, albumentations\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-1.1.0 opencv-python-headless-4.5.5.64 qudida-0.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for import albumentations as A\n",
        "!pip install \"opencv-python-headless<4.3\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2_VhO_M-As3",
        "outputId": "dafc5841-43d2-405f-99da-11e900b44ad4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opencv-python-headless<4.3\n",
            "  Downloading opencv_python_headless-4.2.0.34-cp37-cp37m-manylinux1_x86_64.whl (21.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.6 MB 60.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python-headless<4.3) (1.21.5)\n",
            "Installing collected packages: opencv-python-headless\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.5.5.64\n",
            "    Uninstalling opencv-python-headless-4.5.5.64:\n",
            "      Successfully uninstalled opencv-python-headless-4.5.5.64\n",
            "Successfully installed opencv-python-headless-4.2.0.34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2\n",
        "import glob\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from itertools import chain"
      ],
      "metadata": {
        "id": "v02BaxOpmWXr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Enabling GPU"
      ],
      "metadata": {
        "id": "cBA2e2NrmwYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#print(torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "id": "UajdMgmgn3MR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Download iNaturalist-12K dataset"
      ],
      "metadata": {
        "id": "3i4cMnGWFdrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "actual_data_path = \"/content/drive/MyDrive/inaturalist_12K\""
      ],
      "metadata": {
        "id": "HI4pSHzRGWW5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8672f02e-edbf-4544-f78a-e5073f7acb43"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################\n",
        "#               Define Transforms\n",
        "#######################################################\n",
        "\n",
        "#To define an augmentation pipeline, you need to create an instance of the Compose class.\n",
        "#As an argument to the Compose class, you need to pass a list of augmentations you want to apply. \n",
        "#A call to Compose will return a transform function that will perform image augmentation.\n",
        "#(https://albumentations.ai/docs/getting_started/image_augmentation/)\n",
        "\n",
        "train_transforms = A.Compose(\n",
        "    [\n",
        "        A.SmallestMaxSize(max_size=350),\n",
        "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=360, p=0.5),\n",
        "        A.RandomCrop(height=256, width=256),\n",
        "        A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n",
        "        A.RandomBrightnessContrast(p=0.5),\n",
        "        A.MultiplicativeNoise(multiplier=[0.5,2], per_channel=True, p=0.2),\n",
        "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "        A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
        "        A.RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
        "        ToTensorV2(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "test_transforms = A.Compose(\n",
        "    [\n",
        "        A.SmallestMaxSize(max_size=350),\n",
        "        A.CenterCrop(height=256, width=256),\n",
        "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "        ToTensorV2(),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "xRlQ0er57Vbi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################\n",
        "#       Create Train, Valid and Test sets\n",
        "####################################################\n",
        "train_data_path = os.path.join(actual_data_path, \"train\")\n",
        "test_data_path = os.path.join(actual_data_path, \"val\")\n",
        "\n",
        "train_image_paths = [] #to store image paths in list\n",
        "classes = [] #to store class values\n",
        "\n",
        "#1.\n",
        "# get all the paths from train_data_path and append image paths and class to to respective lists\n",
        "# eg. train path-> 'images/train/26.Pont_du_Gard/4321ee6695c23c7b.jpg'\n",
        "# eg. class -> 26.Pont_du_Gard\n",
        "for data_path in glob.glob(train_data_path + \"/*\"):\n",
        "  classes.append(data_path.split('/')[-1]) \n",
        "  train_image_paths.append(glob.glob(data_path + '/*'))\n",
        "train_image_paths = list(chain.from_iterable(train_image_paths))\n",
        "random.shuffle(train_image_paths)\n",
        "\n",
        "print(len(train_image_paths))\n",
        "print('train_image_path example: ', train_image_paths[2])\n",
        "print('class example: ', classes[2])\n",
        "\n",
        "#2.\n",
        "# split train valid from train paths (80,20)\n",
        "train_image_paths, valid_image_paths = train_image_paths[:int(0.9*len(train_image_paths))], train_image_paths[int(0.9*len(train_image_paths)):] \n",
        "\n",
        "#3.\n",
        "# create the test_image_paths\n",
        "test_image_paths = []\n",
        "for data_path in glob.glob(test_data_path + '/*'):\n",
        "    test_image_paths.append(glob.glob(data_path + '/*'))\n",
        "\n",
        "test_image_paths = list(chain.from_iterable(test_image_paths))\n",
        "\n",
        "print(\"Train size: {}\\nValid size: {}\\nTest size: {}\".format(len(train_image_paths), len(valid_image_paths), len(test_image_paths)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ora_zBPn7o1i",
        "outputId": "e83539e6-481c-40db-83d6-d8a098248fff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10042\n",
            "train_image_path example:  /content/drive/MyDrive/inaturalist_12K/train/Reptilia/95261cb10b22f1d91d129fe85728cd09.jpg\n",
            "class example:  Fungi\n",
            "Train size: 9037\n",
            "Valid size: 1005\n",
            "Test size: 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################\n",
        "#      Create dictionary for class indexes\n",
        "#######################################################\n",
        "\n",
        "idx_to_class = {i:j for i, j in enumerate(classes)}\n",
        "class_to_idx = {value:key for key,value in idx_to_class.items()}"
      ],
      "metadata": {
        "id": "8CxrpoqISjgI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_to_idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jtnJOkzTOyH",
        "outputId": "95bab426-eb2a-45f6-ca19-2edbf8566650"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Amphibia': 1,\n",
              " 'Animalia': 3,\n",
              " 'Arachnida': 0,\n",
              " 'Aves': 7,\n",
              " 'Fungi': 2,\n",
              " 'Insecta': 8,\n",
              " 'Mammalia': 5,\n",
              " 'Mollusca': 4,\n",
              " 'Plantae': 6,\n",
              " 'Reptilia': 9}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################\n",
        "#               Define Dataset Class\n",
        "#######################################################\n",
        "\n",
        "class iNaturalist_12KDataset(Dataset):\n",
        "    def __init__(self, image_paths, transform=False):\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_filepath = self.image_paths[idx]\n",
        "        image = cv2.imread(image_filepath)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        label = image_filepath.split('/')[-2]\n",
        "        label = class_to_idx[label]\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image=image)[\"image\"]\n",
        "        \n",
        "        return image, label\n",
        "    \n"
      ],
      "metadata": {
        "id": "gZs1aTpeVCst"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################\n",
        "#                  Create Dataset\n",
        "#######################################################\n",
        "\n",
        "train_dataset = iNaturalist_12KDataset(train_image_paths,train_transforms)\n",
        "valid_dataset = iNaturalist_12KDataset(valid_image_paths,test_transforms) #test transforms are applied\n",
        "test_dataset = iNaturalist_12KDataset(test_image_paths,test_transforms)"
      ],
      "metadata": {
        "id": "YnBnjzHAVb8M"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset.__len__())\n",
        "# for i in range(train_dataset.__len__()):\n",
        "#   print('The shape of tensor for 50th image in train dataset: ',train_dataset[i][0].shape)\n",
        "#   print('The label for 50th image in train dataset: ',train_dataset[i][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3UO1W7yV387",
        "outputId": "61fb9f3e-7421-42d5-c839-8efa7c725139"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "#######################################################\n",
        "#                  Visualize Dataset\n",
        "#         Images are plotted after augmentation\n",
        "#######################################################\n",
        "\n",
        "def visualize_augmentations(dataset, idx=0, samples=10, cols=5, random_img = False):\n",
        "    \n",
        "    dataset = copy.deepcopy(dataset)\n",
        "    #we remove the normalize and tensor conversion from our augmentation pipeline\n",
        "    dataset.transform = A.Compose([t for t in dataset.transform if not isinstance(t, (A.Normalize, ToTensorV2))])\n",
        "    rows = samples // cols\n",
        "    \n",
        "        \n",
        "    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(12, 8))\n",
        "    for i in range(samples):\n",
        "        if random_img:\n",
        "            idx = np.random.randint(1,len(train_image_paths))\n",
        "        image, lab = dataset[idx]\n",
        "        ax.ravel()[i].imshow(image)\n",
        "        ax.ravel()[i].set_axis_off()\n",
        "        ax.ravel()[i].set_title(idx_to_class[lab])\n",
        "    plt.tight_layout(pad=1)\n",
        "    plt.show()    \n",
        "\n",
        "visualize_augmentations(train_dataset,np.random.randint(1,len(train_image_paths)), random_img = True)\n"
      ],
      "metadata": {
        "id": "VEb6TwA9WJnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################\n",
        "#                  Define Dataloaders\n",
        "#######################################################\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=64, shuffle=True\n",
        ")\n",
        "\n",
        "valid_loader = DataLoader(\n",
        "    valid_dataset, batch_size=64, shuffle=True\n",
        ")\n",
        "\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, batch_size=64, shuffle=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "v07trwTOXqrI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Model"
      ],
      "metadata": {
        "id": "y53VI2iPpO7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CnnModel(nn.Module):\n",
        "  def __init__(self, conv_attributes, pool_attributes,in_feature):\n",
        "    # print(\"PRITHAJ 0---------------\")\n",
        "    super(CnnModel, self).__init__()\n",
        "    self.conv = []\n",
        "    self.pool = []\n",
        "    for i in range(5):\n",
        "      self.conv.append(nn.Conv2d(conv_attributes[i][\"in_channels\"], conv_attributes[i][\"out_channels\"], conv_attributes[i][\"kernel_size\"]))\n",
        "      self.pool.append(nn.MaxPool2d(pool_attributes[i][\"kernel_size\"], pool_attributes[i][\"stride\"]))\n",
        "    self.fc1 = nn.Linear(in_feature, 10)\n",
        "\n",
        "    # self.conv1 = nn.Conv2d(conv_attributes[0][\"in_channels\"], conv_attributes[0][\"out_channels\"], conv_attributes[0][\"kernel_size\"])\n",
        "    # # print(\"conv1 shape\",self.conv1.size)\n",
        "    # self.pool = nn.MaxPool2d(2, 2)\n",
        "    # # print(self.pool.shape)\n",
        "    # self.conv2 = nn.Conv2d(conv_attributes[1][\"in_channels\"], conv_attributes[1][\"out_channels\"], conv_attributes[1][\"kernel_size\"])\n",
        "    # # print(\"conv2 shape\",self.conv2.size)\n",
        "    # self.conv3 = nn.Conv2d(conv_attributes[2][\"in_channels\"], conv_attributes[2][\"out_channels\"], conv_attributes[2][\"kernel_size\"])\n",
        "    # # print(\"conv3 shape\",self.conv2.size)\n",
        "    # self.conv4 = nn.Conv2d(conv_attributes[3][\"in_channels\"], conv_attributes[3][\"out_channels\"], conv_attributes[3][\"kernel_size\"])\n",
        "    # # print(\"conv4 shape\",self.conv2.size)\n",
        "    # self.conv5 = nn.Conv2d(conv_attributes[4][\"in_channels\"], conv_attributes[4][\"out_channels\"], conv_attributes[4][\"kernel_size\"])\n",
        "    # # print(\"conv5 shape\",self.conv2.size)\n",
        "    # input = self.conv5.view(64, -1) # torch.Size([1, 784])\n",
        "    # print(\"input shape\",input.shape)\n",
        "    # print(self.fc1.shape)\n",
        "\n",
        "  def forward(self,x):\n",
        "    # -> n, 3, 32, 32\n",
        "    # print(\"PRITHAJ---------------\")\n",
        "    # print(x.shape)\n",
        "    for i in range(5):\n",
        "      x = self.pool[i](F.relu(self.conv[i](x)))\n",
        "    x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "    x = self.fc1(x)                       # -> n, 10\n",
        "\n",
        "    # x = self.pool(F.relu(self.conv(x)))  # -> n, 6, 14, 14\n",
        "    # # print(\"1\",x.shape)\n",
        "    # x = self.pool(F.relu(self.conv2(x)))  # -> n, 16, 5, 5\n",
        "    # # print(\"2\",x.shape)\n",
        "    # x = self.pool(F.relu(self.conv3(x)))  # -> n, 16, 5, 5\n",
        "    # # print(\"3\",x.shape)\n",
        "    # x = self.pool(F.relu(self.conv4(x)))  # -> n, 16, 5, 5\n",
        "    # # print(\"4\",x.shape)\n",
        "    # x = self.pool(F.relu(self.conv5(x)))  # -> n, 16, 5, 5\n",
        "    # # print(\"5\",x.shape)\n",
        "    # print(\"dense shape\",x.shape)\n",
        "    # x = x.view(-1, 16 * 61 * 61)            # -> n, 400\n",
        "    return x"
      ],
      "metadata": {
        "id": "RxjFGjuXpS_x"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5trpZKH-nIxp"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def TrainNetwork(model,num_epochs, batch_size,learning_rate):\n",
        "  # #model = CNN_Model().to(Device)\n",
        "  # model = CnnModel()\n",
        "  # print(model)\n",
        "  loss_funt = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  n_total_steps = len(train_loader)\n",
        "  for epoch in range(num_epochs):\n",
        "      for i, (images, labels) in enumerate(train_loader):\n",
        "          #print(images)\n",
        "          #print(labels)\n",
        "          # origin shape: [4, 3, 32, 32] = 4, 3, 1024\n",
        "          # input_layer: 3 input channels, 6 output channels, 5 kernel size\n",
        "          #images = images.to(Device)\n",
        "          #labels = labels.to(Device)\n",
        "\n",
        "          # Forward pass\n",
        "          # print(i,images.shape,labels.shape)\n",
        "          outputs = model(images)\n",
        "          loss = loss_funt(outputs, labels)\n",
        "\n",
        "          # Backward and optimize\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          if (i+1) % 20 == 0:\n",
        "              print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "  print('Finished Training')\n",
        "  # PATH = './cnn.pth'\n",
        "  # torch.save(model.state_dict(), PATH)\n",
        "\n"
      ],
      "metadata": {
        "id": "k8ku7nlNhwlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def TestNetwork(model,num_epochs, batch_size,learning_rate):\n",
        "  with torch.no_grad():\n",
        "      n_correct = 0\n",
        "      n_samples = 0\n",
        "      n_class_correct = [0 for i in range(10)]\n",
        "      # print(\"n_class_correct\", n_class_correct)\n",
        "      n_class_samples = [0 for i in range(10)]\n",
        "      # print(\"n_class_correct\",n_class_correct)\n",
        "      for images, labels in test_loader:\n",
        "          #images = images.to(Device)\n",
        "          #labels = labels.to(Device)\n",
        "          # print(\"images.shape\",images.shape)\n",
        "          # print(\"labels.shape\",labels.shape)\n",
        "          outputs = model(images)\n",
        "          # print(\"outputs\",outputs)\n",
        "          # max returns (value ,index)\n",
        "          _, predicted = torch.max(outputs, 1)\n",
        "          # print(\"predicted\",predicted)\n",
        "          n_samples += labels.size(0)\n",
        "          n_correct += (predicted == labels).sum().item()\n",
        "          # print(\"n_samples\",n_samples)\n",
        "          # print(\"n_correct\",n_correct)\n",
        "          # print(\"PREDICTED SIZE\",predicted.size()[0])\n",
        "          for i in range(predicted.size()[0]):\n",
        "              label = labels[i]\n",
        "              # print(\"label\",label)\n",
        "              pred = predicted[i]\n",
        "              # print(\"pred\",pred)\n",
        "              if (label == pred):\n",
        "                  n_class_correct[label] += 1\n",
        "                  # print(\"n_class_correct\",n_class_correct)\n",
        "              n_class_samples[label] += 1\n",
        "              # print(\"n_class_samples\",n_class_samples)\n",
        "\n",
        "      acc = 100.0 * n_correct / n_samples\n",
        "      print(f'Accuracy of the network: {acc} %')\n",
        "\n",
        "      for i in range(10):\n",
        "          acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
        "          print(f'Accuracy of {classes[i]}: {acc} %')"
      ],
      "metadata": {
        "id": "2NfP-wztztIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "g0k_29HqD8Yr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Calculates the input feature for the dense linear layer\n",
        "def LinearInFeatureCalculate(initial_dim,conv_attributes,pool_attributes):\n",
        "  for i in range(5):\n",
        "    D = (initial_dim + 2*conv_attributes[i][\"padding\"] - conv_attributes[i][\"dilation\"]*(conv_attributes[i][\"kernel_size\"]-1) - 1)//(conv_attributes[i][\"stride\"]) + 1\n",
        "    D = D//pool_attributes[i][\"stride\"]\n",
        "    initial_dim = D\n",
        "  return D\n"
      ],
      "metadata": {
        "id": "lBScrogGDsZv"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main function**"
      ],
      "metadata": {
        "id": "KS9k_0LHD9Y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  print(\"Hello\")\n",
        "  conv_attributes = [{\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1},\n",
        "                     {\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1},\n",
        "                     {\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1},\n",
        "                     {\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1},\n",
        "                     {\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1}]\n",
        "  \n",
        "  \n",
        "  ##Attributes for 1st Convolution Layer\n",
        "  conv_attributes[0][\"in_channels\"]=3\n",
        "  conv_attributes[0][\"out_channels\"]=6\n",
        "  conv_attributes[0][\"kernel_size\"]=3\n",
        "\n",
        "  ##Attributes for 2nd Convolution Layer\n",
        "  conv_attributes[1][\"in_channels\"]=6\n",
        "  conv_attributes[1][\"out_channels\"]=12\n",
        "  conv_attributes[1][\"kernel_size\"]=3\n",
        "\n",
        "  ##Attributes for 3rd Convolution Layer\n",
        "  conv_attributes[2][\"in_channels\"]=12\n",
        "  conv_attributes[2][\"out_channels\"]=16\n",
        "  conv_attributes[2][\"kernel_size\"]=5\n",
        "\n",
        "  ##Attributes for 4th Convolution Layer\n",
        "  conv_attributes[3][\"in_channels\"]=16\n",
        "  conv_attributes[3][\"out_channels\"]=32\n",
        "  conv_attributes[3][\"kernel_size\"]=5\n",
        "\n",
        "  ##Attributes for 5th Convolution Layer\n",
        "  conv_attributes[4][\"in_channels\"]=32\n",
        "  conv_attributes[4][\"out_channels\"]=64\n",
        "  conv_attributes[4][\"kernel_size\"]=7\n",
        "\n",
        "  pool_attributes = [{\"kernel_size\":1, \"stride\": 1},\n",
        "                     {\"kernel_size\":1, \"stride\": 1},\n",
        "                     {\"kernel_size\":1, \"stride\": 1},\n",
        "                     {\"kernel_size\":1, \"stride\": 1},\n",
        "                     {\"kernel_size\":1, \"stride\": 1}]\n",
        "\n",
        "  pool_attributes[0][\"kernel_size\"]=2\n",
        "  pool_attributes[0][\"stride\"]=2\n",
        "\n",
        "  pool_attributes[1][\"kernel_size\"]=2\n",
        "  pool_attributes[1][\"stride\"]=2\n",
        "  \n",
        "  pool_attributes[2][\"kernel_size\"]=2\n",
        "  pool_attributes[2][\"stride\"]=2\n",
        "\n",
        "  pool_attributes[3][\"kernel_size\"]=2\n",
        "  pool_attributes[3][\"stride\"]=2\n",
        "\n",
        "  pool_attributes[4][\"kernel_size\"]=2\n",
        "  pool_attributes[4][\"stride\"]=2\n",
        "\n",
        "  final_dim=LinearInFeatureCalculate(256,conv_attributes,pool_attributes)\n",
        "\n",
        "  in_feature = (final_dim ** 2) * conv_attributes[4][\"out_channels\"]\n",
        "  print(in_feature)\n",
        "\n",
        "  #model = CNN_Model().to(Device)\n",
        "  model = CnnModel(conv_attributes, pool_attributes,in_feature)"
      ],
      "metadata": {
        "id": "pejeGIEMD87w"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if  __name__ ==\"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "dxEPk_H5FT4_",
        "outputId": "e3e0c875-4999-4c29-ae53-cd513d8f889c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "576\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-ca589dfed59f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m  \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-3718e939f82e>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m   \u001b[0;31m#model = CNN_Model().to(Device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCnnModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_attributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0min_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'CnnModel' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hlFB5Hu3Zgar"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}