{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9oMWDx10UUx"
      },
      "source": [
        "##ASSIGNMENT-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQnccrq2ewXw"
      },
      "source": [
        "Learn how to use CNNs: train from scratch, finetune a pretrained model, use a pre-trained model as it is.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vfgl_cijfI9g"
      },
      "source": [
        "##Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpDHokXY8ffl",
        "outputId": "e41e0a80-d7c5-4e36-8393-fe44064f4268"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.7/dist-packages (0.1.12)\n",
            "Collecting albumentations\n",
            "  Downloading albumentations-1.1.0-py3-none-any.whl (102 kB)\n",
            "\u001b[K     |████████████████████████████████| 102 kB 17.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.21.5)\n",
            "Collecting opencv-python-headless>=4.1.1\n",
            "  Downloading opencv_python_headless-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.8 MB 2.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.4.1)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (0.18.3)\n",
            "Collecting qudida>=0.0.4\n",
            "  Downloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations) (3.13)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations) (3.10.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations) (1.0.2)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (3.2.2)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2021.11.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.4.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (7.1.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.6.3)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (3.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.1.0)\n",
            "Installing collected packages: opencv-python-headless, qudida, albumentations\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-1.1.0 opencv-python-headless-4.5.5.64 qudida-0.0.4\n"
          ]
        }
      ],
      "source": [
        "!pip install -U albumentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2_VhO_M-As3",
        "outputId": "dafc5841-43d2-405f-99da-11e900b44ad4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting opencv-python-headless<4.3\n",
            "  Downloading opencv_python_headless-4.2.0.34-cp37-cp37m-manylinux1_x86_64.whl (21.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.6 MB 60.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python-headless<4.3) (1.21.5)\n",
            "Installing collected packages: opencv-python-headless\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.5.5.64\n",
            "    Uninstalling opencv-python-headless-4.5.5.64:\n",
            "      Successfully uninstalled opencv-python-headless-4.5.5.64\n",
            "Successfully installed opencv-python-headless-4.2.0.34\n"
          ]
        }
      ],
      "source": [
        "#for import albumentations as A\n",
        "!pip install \"opencv-python-headless<4.3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "v02BaxOpmWXr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2\n",
        "import glob\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from itertools import chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBA2e2NrmwYD"
      },
      "source": [
        "##Enabling GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UajdMgmgn3MR"
      },
      "outputs": [],
      "source": [
        "#Device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#print(torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3i4cMnGWFdrG"
      },
      "source": [
        "##Download iNaturalist-12K dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HI4pSHzRGWW5",
        "outputId": "8672f02e-edbf-4544-f78a-e5073f7acb43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "actual_data_path = \"/content/drive/MyDrive/inaturalist_12K\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xRlQ0er57Vbi"
      },
      "outputs": [],
      "source": [
        "#######################################################\n",
        "#               Define Transforms\n",
        "#######################################################\n",
        "\n",
        "#To define an augmentation pipeline, you need to create an instance of the Compose class.\n",
        "#As an argument to the Compose class, you need to pass a list of augmentations you want to apply. \n",
        "#A call to Compose will return a transform function that will perform image augmentation.\n",
        "#(https://albumentations.ai/docs/getting_started/image_augmentation/)\n",
        "\n",
        "train_transforms = A.Compose(\n",
        "    [\n",
        "        A.SmallestMaxSize(max_size=350),\n",
        "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=360, p=0.5),\n",
        "        A.RandomCrop(height=256, width=256),\n",
        "        A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n",
        "        A.RandomBrightnessContrast(p=0.5),\n",
        "        A.MultiplicativeNoise(multiplier=[0.5,2], per_channel=True, p=0.2),\n",
        "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "        A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
        "        A.RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
        "        ToTensorV2(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "test_transforms = A.Compose(\n",
        "    [\n",
        "        A.SmallestMaxSize(max_size=350),\n",
        "        A.CenterCrop(height=256, width=256),\n",
        "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "        ToTensorV2(),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ora_zBPn7o1i",
        "outputId": "e83539e6-481c-40db-83d6-d8a098248fff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10042\n",
            "train_image_path example:  /content/drive/MyDrive/inaturalist_12K/train/Reptilia/95261cb10b22f1d91d129fe85728cd09.jpg\n",
            "class example:  Fungi\n",
            "Train size: 9037\n",
            "Valid size: 1005\n",
            "Test size: 2000\n"
          ]
        }
      ],
      "source": [
        "####################################################\n",
        "#       Create Train, Valid and Test sets\n",
        "####################################################\n",
        "train_data_path = os.path.join(actual_data_path, \"train\")\n",
        "test_data_path = os.path.join(actual_data_path, \"val\")\n",
        "\n",
        "train_image_paths = [] #to store image paths in list\n",
        "classes = [] #to store class values\n",
        "\n",
        "#1.\n",
        "# get all the paths from train_data_path and append image paths and class to to respective lists\n",
        "# eg. train path-> 'images/train/26.Pont_du_Gard/4321ee6695c23c7b.jpg'\n",
        "# eg. class -> 26.Pont_du_Gard\n",
        "for data_path in glob.glob(train_data_path + \"/*\"):\n",
        "  classes.append(data_path.split('/')[-1]) \n",
        "  train_image_paths.append(glob.glob(data_path + '/*'))\n",
        "train_image_paths = list(chain.from_iterable(train_image_paths))\n",
        "random.shuffle(train_image_paths)\n",
        "\n",
        "print(len(train_image_paths))\n",
        "print('train_image_path example: ', train_image_paths[2])\n",
        "print('class example: ', classes[2])\n",
        "\n",
        "#2.\n",
        "# split train valid from train paths (80,20)\n",
        "train_image_paths, valid_image_paths = train_image_paths[:int(0.9*len(train_image_paths))], train_image_paths[int(0.9*len(train_image_paths)):] \n",
        "\n",
        "#3.\n",
        "# create the test_image_paths\n",
        "test_image_paths = []\n",
        "for data_path in glob.glob(test_data_path + '/*'):\n",
        "    test_image_paths.append(glob.glob(data_path + '/*'))\n",
        "\n",
        "test_image_paths = list(chain.from_iterable(test_image_paths))\n",
        "\n",
        "print(\"Train size: {}\\nValid size: {}\\nTest size: {}\".format(len(train_image_paths), len(valid_image_paths), len(test_image_paths)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8CxrpoqISjgI"
      },
      "outputs": [],
      "source": [
        "#######################################################\n",
        "#      Create dictionary for class indexes\n",
        "#######################################################\n",
        "\n",
        "idx_to_class = {i:j for i, j in enumerate(classes)}\n",
        "class_to_idx = {value:key for key,value in idx_to_class.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jtnJOkzTOyH",
        "outputId": "95bab426-eb2a-45f6-ca19-2edbf8566650"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Amphibia': 1,\n",
              " 'Animalia': 3,\n",
              " 'Arachnida': 0,\n",
              " 'Aves': 7,\n",
              " 'Fungi': 2,\n",
              " 'Insecta': 8,\n",
              " 'Mammalia': 5,\n",
              " 'Mollusca': 4,\n",
              " 'Plantae': 6,\n",
              " 'Reptilia': 9}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class_to_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gZs1aTpeVCst"
      },
      "outputs": [],
      "source": [
        "#######################################################\n",
        "#               Define Dataset Class\n",
        "#######################################################\n",
        "\n",
        "class iNaturalist_12KDataset(Dataset):\n",
        "    def __init__(self, image_paths, transform=False):\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_filepath = self.image_paths[idx]\n",
        "        image = cv2.imread(image_filepath)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        label = image_filepath.split('/')[-2]\n",
        "        label = class_to_idx[label]\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image=image)[\"image\"]\n",
        "        \n",
        "        return image, label\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YnBnjzHAVb8M"
      },
      "outputs": [],
      "source": [
        "#######################################################\n",
        "#                  Create Dataset\n",
        "#######################################################\n",
        "\n",
        "train_dataset = iNaturalist_12KDataset(train_image_paths,train_transforms)\n",
        "valid_dataset = iNaturalist_12KDataset(valid_image_paths,test_transforms) #test transforms are applied\n",
        "test_dataset = iNaturalist_12KDataset(test_image_paths,test_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3UO1W7yV387",
        "outputId": "61fb9f3e-7421-42d5-c839-8efa7c725139"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9037\n"
          ]
        }
      ],
      "source": [
        "print(train_dataset.__len__())\n",
        "# for i in range(train_dataset.__len__()):\n",
        "#   print('The shape of tensor for 50th image in train dataset: ',train_dataset[i][0].shape)\n",
        "#   print('The label for 50th image in train dataset: ',train_dataset[i][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEb6TwA9WJnz"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "#######################################################\n",
        "#                  Visualize Dataset\n",
        "#         Images are plotted after augmentation\n",
        "#######################################################\n",
        "\n",
        "def visualize_augmentations(dataset, idx=0, samples=10, cols=5, random_img = False):\n",
        "    \n",
        "    dataset = copy.deepcopy(dataset)\n",
        "    #we remove the normalize and tensor conversion from our augmentation pipeline\n",
        "    dataset.transform = A.Compose([t for t in dataset.transform if not isinstance(t, (A.Normalize, ToTensorV2))])\n",
        "    rows = samples // cols\n",
        "    \n",
        "        \n",
        "    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(12, 8))\n",
        "    for i in range(samples):\n",
        "        if random_img:\n",
        "            idx = np.random.randint(1,len(train_image_paths))\n",
        "        image, lab = dataset[idx]\n",
        "        ax.ravel()[i].imshow(image)\n",
        "        ax.ravel()[i].set_axis_off()\n",
        "        ax.ravel()[i].set_title(idx_to_class[lab])\n",
        "    plt.tight_layout(pad=1)\n",
        "    plt.show()    \n",
        "\n",
        "visualize_augmentations(train_dataset,np.random.randint(1,len(train_image_paths)), random_img = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "v07trwTOXqrI"
      },
      "outputs": [],
      "source": [
        "#######################################################\n",
        "#                  Define Dataloaders\n",
        "#######################################################\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=64, shuffle=True\n",
        ")\n",
        "\n",
        "valid_loader = DataLoader(\n",
        "    valid_dataset, batch_size=64, shuffle=True\n",
        ")\n",
        "\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, batch_size=64, shuffle=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y53VI2iPpO7k"
      },
      "source": [
        "## Building the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "RxjFGjuXpS_x"
      },
      "outputs": [],
      "source": [
        "class CnnModel(nn.Module):\n",
        "  def __init__(self, conv_attributes, pool_attributes,in_feature):\n",
        "    # print(\"PRITHAJ 0---------------\")\n",
        "    super(CnnModel, self).__init__()\n",
        "    self.conv = []\n",
        "    self.pool = []\n",
        "    for i in range(5):\n",
        "      self.conv.append(nn.Conv2d(conv_attributes[i][\"in_channels\"], conv_attributes[i][\"out_channels\"], conv_attributes[i][\"kernel_size\"]))\n",
        "      self.pool.append(nn.MaxPool2d(pool_attributes[i][\"kernel_size\"], pool_attributes[i][\"stride\"]))\n",
        "    self.fc1 = nn.Linear(in_feature, 10)\n",
        "\n",
        "    # self.conv1 = nn.Conv2d(conv_attributes[0][\"in_channels\"], conv_attributes[0][\"out_channels\"], conv_attributes[0][\"kernel_size\"])\n",
        "    # # print(\"conv1 shape\",self.conv1.size)\n",
        "    # self.pool = nn.MaxPool2d(2, 2)\n",
        "    # # print(self.pool.shape)\n",
        "    # self.conv2 = nn.Conv2d(conv_attributes[1][\"in_channels\"], conv_attributes[1][\"out_channels\"], conv_attributes[1][\"kernel_size\"])\n",
        "    # # print(\"conv2 shape\",self.conv2.size)\n",
        "    # self.conv3 = nn.Conv2d(conv_attributes[2][\"in_channels\"], conv_attributes[2][\"out_channels\"], conv_attributes[2][\"kernel_size\"])\n",
        "    # # print(\"conv3 shape\",self.conv2.size)\n",
        "    # self.conv4 = nn.Conv2d(conv_attributes[3][\"in_channels\"], conv_attributes[3][\"out_channels\"], conv_attributes[3][\"kernel_size\"])\n",
        "    # # print(\"conv4 shape\",self.conv2.size)\n",
        "    # self.conv5 = nn.Conv2d(conv_attributes[4][\"in_channels\"], conv_attributes[4][\"out_channels\"], conv_attributes[4][\"kernel_size\"])\n",
        "    # # print(\"conv5 shape\",self.conv2.size)\n",
        "    # input = self.conv5.view(64, -1) # torch.Size([1, 784])\n",
        "    # print(\"input shape\",input.shape)\n",
        "    # print(self.fc1.shape)\n",
        "\n",
        "  def forward(self,x):\n",
        "    # -> n, 3, 32, 32\n",
        "    # print(\"PRITHAJ---------------\")\n",
        "    # print(x.shape)\n",
        "    for i in range(5):\n",
        "      x = self.pool[i](F.relu(self.conv[i](x)))\n",
        "    x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "    x = self.fc1(x)                       # -> n, 10\n",
        "\n",
        "    # x = self.pool(F.relu(self.conv(x)))  # -> n, 6, 14, 14\n",
        "    # # print(\"1\",x.shape)\n",
        "    # x = self.pool(F.relu(self.conv2(x)))  # -> n, 16, 5, 5\n",
        "    # # print(\"2\",x.shape)\n",
        "    # x = self.pool(F.relu(self.conv3(x)))  # -> n, 16, 5, 5\n",
        "    # # print(\"3\",x.shape)\n",
        "    # x = self.pool(F.relu(self.conv4(x)))  # -> n, 16, 5, 5\n",
        "    # # print(\"4\",x.shape)\n",
        "    # x = self.pool(F.relu(self.conv5(x)))  # -> n, 16, 5, 5\n",
        "    # # print(\"5\",x.shape)\n",
        "    # print(\"dense shape\",x.shape)\n",
        "    # x = x.view(-1, 16 * 61 * 61)            # -> n, 400\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "5trpZKH-nIxp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8ku7nlNhwlc"
      },
      "outputs": [],
      "source": [
        "def TrainNetwork(model,num_epochs, batch_size,learning_rate):\n",
        "  # #model = CNN_Model().to(Device)\n",
        "  # model = CnnModel()\n",
        "  # print(model)\n",
        "  loss_funt = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  n_total_steps = len(train_loader)\n",
        "  for epoch in range(num_epochs):\n",
        "      for i, (images, labels) in enumerate(train_loader):\n",
        "          #print(images)\n",
        "          #print(labels)\n",
        "          # origin shape: [4, 3, 32, 32] = 4, 3, 1024\n",
        "          # input_layer: 3 input channels, 6 output channels, 5 kernel size\n",
        "          #images = images.to(Device)\n",
        "          #labels = labels.to(Device)\n",
        "\n",
        "          # Forward pass\n",
        "          # print(i,images.shape,labels.shape)\n",
        "          outputs = model(images)\n",
        "          loss = loss_funt(outputs, labels)\n",
        "\n",
        "          # Backward and optimize\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          if (i+1) % 20 == 0:\n",
        "              print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "  print('Finished Training')\n",
        "  # PATH = './cnn.pth'\n",
        "  # torch.save(model.state_dict(), PATH)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NfP-wztztIv"
      },
      "outputs": [],
      "source": [
        "def TestNetwork(model,num_epochs, batch_size,learning_rate):\n",
        "  with torch.no_grad():\n",
        "      n_correct = 0\n",
        "      n_samples = 0\n",
        "      n_class_correct = [0 for i in range(10)]\n",
        "      # print(\"n_class_correct\", n_class_correct)\n",
        "      n_class_samples = [0 for i in range(10)]\n",
        "      # print(\"n_class_correct\",n_class_correct)\n",
        "      for images, labels in test_loader:\n",
        "          #images = images.to(Device)\n",
        "          #labels = labels.to(Device)\n",
        "          # print(\"images.shape\",images.shape)\n",
        "          # print(\"labels.shape\",labels.shape)\n",
        "          outputs = model(images)\n",
        "          # print(\"outputs\",outputs)\n",
        "          # max returns (value ,index)\n",
        "          _, predicted = torch.max(outputs, 1)\n",
        "          # print(\"predicted\",predicted)\n",
        "          n_samples += labels.size(0)\n",
        "          n_correct += (predicted == labels).sum().item()\n",
        "          # print(\"n_samples\",n_samples)\n",
        "          # print(\"n_correct\",n_correct)\n",
        "          # print(\"PREDICTED SIZE\",predicted.size()[0])\n",
        "          for i in range(predicted.size()[0]):\n",
        "              label = labels[i]\n",
        "              # print(\"label\",label)\n",
        "              pred = predicted[i]\n",
        "              # print(\"pred\",pred)\n",
        "              if (label == pred):\n",
        "                  n_class_correct[label] += 1\n",
        "                  # print(\"n_class_correct\",n_class_correct)\n",
        "              n_class_samples[label] += 1\n",
        "              # print(\"n_class_samples\",n_class_samples)\n",
        "\n",
        "      acc = 100.0 * n_correct / n_samples\n",
        "      print(f'Accuracy of the network: {acc} %')\n",
        "\n",
        "      for i in range(10):\n",
        "          acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
        "          print(f'Accuracy of {classes[i]}: {acc} %')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "lBScrogGDsZv"
      },
      "outputs": [],
      "source": [
        "##Calculates the input feature for the dense linear layer\n",
        "def LinearInFeatureCalculate(initial_dim,conv_attributes,pool_attributes):\n",
        "  for i in range(5):\n",
        "    D = (initial_dim + 2*conv_attributes[i][\"padding\"] - conv_attributes[i][\"dilation\"]*(conv_attributes[i][\"kernel_size\"]-1) - 1)//(conv_attributes[i][\"stride\"]) + 1\n",
        "    D = D//pool_attributes[i][\"stride\"]\n",
        "    initial_dim = D\n",
        "  return D\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS9k_0LHD9Y6"
      },
      "source": [
        "**Main function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "pejeGIEMD87w"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "  print(\"Hello\")\n",
        "  conv_attributes = [{\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1},\n",
        "                     {\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1},\n",
        "                     {\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1},\n",
        "                     {\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1},\n",
        "                     {\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1}]\n",
        "  \n",
        "  \n",
        "  ##Attributes for 1st Convolution Layer\n",
        "  conv_attributes[0][\"in_channels\"]=3\n",
        "  conv_attributes[0][\"out_channels\"]=6\n",
        "  conv_attributes[0][\"kernel_size\"]=3\n",
        "\n",
        "  ##Attributes for 2nd Convolution Layer\n",
        "  conv_attributes[1][\"in_channels\"]=6\n",
        "  conv_attributes[1][\"out_channels\"]=12\n",
        "  conv_attributes[1][\"kernel_size\"]=3\n",
        "\n",
        "  ##Attributes for 3rd Convolution Layer\n",
        "  conv_attributes[2][\"in_channels\"]=12\n",
        "  conv_attributes[2][\"out_channels\"]=16\n",
        "  conv_attributes[2][\"kernel_size\"]=5\n",
        "\n",
        "  ##Attributes for 4th Convolution Layer\n",
        "  conv_attributes[3][\"in_channels\"]=16\n",
        "  conv_attributes[3][\"out_channels\"]=32\n",
        "  conv_attributes[3][\"kernel_size\"]=5\n",
        "\n",
        "  ##Attributes for 5th Convolution Layer\n",
        "  conv_attributes[4][\"in_channels\"]=32\n",
        "  conv_attributes[4][\"out_channels\"]=64\n",
        "  conv_attributes[4][\"kernel_size\"]=7\n",
        "\n",
        "  pool_attributes = [{\"kernel_size\":1, \"stride\": 1},\n",
        "                     {\"kernel_size\":1, \"stride\": 1},\n",
        "                     {\"kernel_size\":1, \"stride\": 1},\n",
        "                     {\"kernel_size\":1, \"stride\": 1},\n",
        "                     {\"kernel_size\":1, \"stride\": 1}]\n",
        "\n",
        "  pool_attributes[0][\"kernel_size\"]=2\n",
        "  pool_attributes[0][\"stride\"]=2\n",
        "\n",
        "  pool_attributes[1][\"kernel_size\"]=2\n",
        "  pool_attributes[1][\"stride\"]=2\n",
        "  \n",
        "  pool_attributes[2][\"kernel_size\"]=2\n",
        "  pool_attributes[2][\"stride\"]=2\n",
        "\n",
        "  pool_attributes[3][\"kernel_size\"]=2\n",
        "  pool_attributes[3][\"stride\"]=2\n",
        "\n",
        "  pool_attributes[4][\"kernel_size\"]=2\n",
        "  pool_attributes[4][\"stride\"]=2\n",
        "\n",
        "  final_dim=LinearInFeatureCalculate(256,conv_attributes,pool_attributes)\n",
        "\n",
        "  in_feature = (final_dim ** 2) * conv_attributes[4][\"out_channels\"]\n",
        "  print(in_feature)\n",
        "\n",
        "  #model = CNN_Model().to(Device)\n",
        "  model = CnnModel(conv_attributes, pool_attributes,in_feature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "dxEPk_H5FT4_",
        "outputId": "e3e0c875-4999-4c29-ae53-cd513d8f889c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello\n",
            "576\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-ca589dfed59f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m  \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-3718e939f82e>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m   \u001b[0;31m#model = CNN_Model().to(Device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCnnModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_attributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0min_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'CnnModel' is not defined"
          ]
        }
      ],
      "source": [
        "if  __name__ ==\"__main__\":\n",
        "  main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlFB5Hu3Zgar"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "jStq0xoQ4BWn",
        "Tkk4w8VZ4T-0",
        "ovjKy6vrJerT",
        "7CANEN4V89A4"
      ],
      "name": "Pytorch_Assignment2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
