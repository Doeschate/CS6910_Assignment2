{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch_Assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jStq0xoQ4BWn",
        "Tkk4w8VZ4T-0",
        "ovjKy6vrJerT",
        "7CANEN4V89A4"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##ASSIGNMENT-2"
      ],
      "metadata": {
        "id": "i9oMWDx10UUx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learn how to use CNNs: train from scratch, finetune a pretrained model, use a pre-trained model as it is.\n"
      ],
      "metadata": {
        "id": "CQnccrq2ewXw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installs**"
      ],
      "metadata": {
        "id": "uifpKNB1odVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U albumentations\n",
        "!pip install \"opencv-python-headless<4.3\" #for import albumentations as A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAGlgBb8okJh",
        "outputId": "990448fa-10df-4dc0-d0b2-b0eeb63762f9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.7/dist-packages (0.1.12)\n",
            "Collecting albumentations\n",
            "  Downloading albumentations-1.1.0-py3-none-any.whl (102 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▏                            | 10 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 20 kB 38.2 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 30 kB 36.7 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 40 kB 28.3 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 51 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 61 kB 32.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 71 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 81 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 92 kB 29.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102 kB 31.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102 kB 31.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.21.5)\n",
            "Collecting qudida>=0.0.4\n",
            "  Downloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations) (3.13)\n",
            "Collecting opencv-python-headless>=4.1.1\n",
            "  Downloading opencv_python_headless-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.8 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (0.18.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations) (1.0.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations) (3.10.0.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.4.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (7.1.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (1.3.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.6.3)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (3.2.2)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2021.11.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (3.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.1.0)\n",
            "Installing collected packages: opencv-python-headless, qudida, albumentations\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-1.1.0 opencv-python-headless-4.5.5.64 qudida-0.0.4\n",
            "Collecting opencv-python-headless<4.3\n",
            "  Downloading opencv_python_headless-4.2.0.34-cp37-cp37m-manylinux1_x86_64.whl (21.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.6 MB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python-headless<4.3) (1.21.5)\n",
            "Installing collected packages: opencv-python-headless\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.5.5.64\n",
            "    Uninstalling opencv-python-headless-4.5.5.64:\n",
            "      Successfully uninstalled opencv-python-headless-4.5.5.64\n",
            "Successfully installed opencv-python-headless-4.2.0.34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imports**"
      ],
      "metadata": {
        "id": "Vfgl_cijfI9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2\n",
        "import glob\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import chain\n",
        "enable_GPU = 0"
      ],
      "metadata": {
        "id": "qpDHokXY8ffl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enabling GPU**"
      ],
      "metadata": {
        "id": "cBA2e2NrmwYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(torch.cuda.get_device_name(0))\n",
        "enable_GPU = 1"
      ],
      "metadata": {
        "id": "UajdMgmgn3MR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2691500-f008-40ac-d890-ac2f13b4d793"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download iNaturalist-12K dataset**"
      ],
      "metadata": {
        "id": "3i4cMnGWFdrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "HI4pSHzRGWW5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99870b60-4005-46a5-e0dd-2e2eef9c51f8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Creating**"
      ],
      "metadata": {
        "id": "DgxaBE_29k3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get all the paths from train_data_path and returns image paths for train and validation set\n",
        "def CreateTrainDataset(actual_data_path):\n",
        "  train_data_path = os.path.join(actual_data_path, \"train\")\n",
        "  train_image_paths = [] #to store image paths in list\n",
        "  classes = [] #to store class values\n",
        "  for data_path in glob.glob(train_data_path + \"/*\"):\n",
        "    classes.append(data_path.split('/')[-1]) \n",
        "    train_image_paths.append(glob.glob(data_path + '/*'))\n",
        "  train_image_paths = list(chain.from_iterable(train_image_paths))\n",
        "  random.shuffle(train_image_paths)\n",
        "\n",
        "  # split train valid from train paths (90,10)\n",
        "  train_image_paths, valid_image_paths = train_image_paths[:int(0.9*len(train_image_paths))], train_image_paths[int(0.9*len(train_image_paths)):] \n",
        "  return train_image_paths, valid_image_paths\n",
        "\n",
        "# create the test_image_paths\n",
        "def CreateTestDataset(actual_data_path):\n",
        "  test_data_path = os.path.join(actual_data_path, \"val\")\n",
        "  test_image_paths = []\n",
        "  for data_path in glob.glob(test_data_path + '/*'):\n",
        "      test_image_paths.append(glob.glob(data_path + '/*'))\n",
        "  test_image_paths = list(chain.from_iterable(test_image_paths))\n",
        "  return test_image_paths"
      ],
      "metadata": {
        "id": "ora_zBPn7o1i"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create dictionary for class indexes\n",
        "actual_data_path = \"/content/drive/MyDrive/inaturalist_12K\"\n",
        "train_data_path = os.path.join(actual_data_path, \"train\")\n",
        "classes = [] #to store class values\n",
        "for data_path in glob.glob(train_data_path + \"/*\"):\n",
        "  classes.append(data_path.split('/')[-1])\n",
        "idx_to_class = {i:j for i, j in enumerate(classes)}\n",
        "class_to_idx = {value:key for key,value in idx_to_class.items()}"
      ],
      "metadata": {
        "id": "8CxrpoqISjgI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_to_idx"
      ],
      "metadata": {
        "id": "2jtnJOkzTOyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Class**"
      ],
      "metadata": {
        "id": "-B7uy3GTY1gi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class iNaturalist_12KDataset(Dataset):\n",
        "    def __init__(self, image_paths, transform=False):\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_filepath = self.image_paths[idx]\n",
        "        image = cv2.imread(image_filepath)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        label = image_filepath.split('/')[-2]\n",
        "        label = class_to_idx[label]\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image=image)[\"image\"]\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "gZs1aTpeVCst"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building the Model**"
      ],
      "metadata": {
        "id": "y53VI2iPpO7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CnnModel(nn.Module):\n",
        "  def __init__(self, conv_attributes, pool_attributes,in_feature):\n",
        "    super(CnnModel, self).__init__()\n",
        "    self.conv1= nn.Conv2d(conv_attributes[0][\"in_channels\"], conv_attributes[0][\"out_channels\"], conv_attributes[0][\"kernel_size\"])\n",
        "    self.pool1= nn.MaxPool2d(pool_attributes[0][\"kernel_size\"], pool_attributes[0][\"stride\"])\n",
        "\n",
        "    self.conv2= nn.Conv2d(conv_attributes[1][\"in_channels\"], conv_attributes[1][\"out_channels\"], conv_attributes[1][\"kernel_size\"])\n",
        "    self.pool2= nn.MaxPool2d(pool_attributes[1][\"kernel_size\"], pool_attributes[1][\"stride\"])\n",
        "\n",
        "    self.conv3= nn.Conv2d(conv_attributes[2][\"in_channels\"], conv_attributes[2][\"out_channels\"], conv_attributes[2][\"kernel_size\"])\n",
        "    self.pool3= nn.MaxPool2d(pool_attributes[2][\"kernel_size\"], pool_attributes[2][\"stride\"])\n",
        "\n",
        "    self.conv4= nn.Conv2d(conv_attributes[3][\"in_channels\"], conv_attributes[3][\"out_channels\"], conv_attributes[3][\"kernel_size\"])\n",
        "    self.pool4= nn.MaxPool2d(pool_attributes[3][\"kernel_size\"], pool_attributes[3][\"stride\"])\n",
        "\n",
        "    self.conv5= nn.Conv2d(conv_attributes[4][\"in_channels\"], conv_attributes[4][\"out_channels\"], conv_attributes[4][\"kernel_size\"])\n",
        "    self.pool5= nn.MaxPool2d(pool_attributes[4][\"kernel_size\"], pool_attributes[4][\"stride\"])\n",
        "\n",
        "    self.fc1 = nn.Linear(in_feature, 10)\n",
        "   \n",
        "  def forward(self,x):\n",
        "    # print(\"FORWARD CHECK\")\n",
        "    x = self.pool1(F.relu(self.conv1(x)))\n",
        "    x = self.pool2(F.relu(self.conv2(x)))\n",
        "    x = self.pool3(F.relu(self.conv3(x)))\n",
        "    x = self.pool4(F.relu(self.conv4(x)))\n",
        "    x = self.pool5(F.relu(self.conv5(x)))\n",
        "\n",
        "    x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "    x = self.fc1(x)                       \n",
        "    return x"
      ],
      "metadata": {
        "id": "-fhgXbvhiExl"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def OptimizerFunction(model, learning_rate, optimizer_name):\n",
        "  if optimizer_name == \"SGD\":\n",
        "    return torch.optim.SGD(model.parameters(), learning_rate)\n",
        "  elif optimizer_name == \"Adam\":\n",
        "    return torch.optim.Adam(model.parameters(), learning_rate)"
      ],
      "metadata": {
        "id": "5trpZKH-nIxp"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LossFunction():\n",
        "  return nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "kb7hUIJfxQHX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def TrainNetwork(model,num_epochs, batch_size,learning_rate,optimizer_name,resized_shape,actual_data_path):\n",
        "  # print(\"TRAINING---------------\")\n",
        "  loss_funt = LossFunction()\n",
        "  optimizer = OptimizerFunction(model, learning_rate, optimizer_name)\n",
        "\n",
        "  #Function for image augmentation.Calling Compose returns a transform function that performs image augmentation.\n",
        "  train_transforms = A.Compose([# A.SmallestMaxSize(max_size=350),\n",
        "            A.Resize(resized_shape,resized_shape),\n",
        "            # A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=360, p=0.5),\n",
        "            # A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n",
        "            A.RandomBrightnessContrast(p=0.5),\n",
        "            # A.MultiplicativeNoise(multiplier=[0.5,2], per_channel=True, p=0.2),\n",
        "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "            # A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
        "            # A.RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
        "            ToTensorV2(),])\n",
        "\n",
        "  #Function to create train, validation dataset and returns the train and validation image paths\n",
        "  train_image_paths, valid_image_paths=CreateTrainDataset(actual_data_path)\n",
        "\n",
        "  #Training Dataset created\n",
        "  train_dataset = iNaturalist_12KDataset(train_image_paths,train_transforms)\n",
        "\n",
        "  #Dataloader loads train dataset\n",
        "  train_loader = DataLoader(\n",
        "      train_dataset, batch_size=batch_size, shuffle=True\n",
        "  )\n",
        "  n_total_steps = len(train_loader)\n",
        "  for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "      if enable_GPU == 1 :\n",
        "        images = images.to(Device)\n",
        "        labels = labels.to(Device)\n",
        "\n",
        "      # Forward pass\n",
        "      # print(i)\n",
        "      outputs = model(images)\n",
        "      # print(outputs)\n",
        "      loss = loss_funt(outputs, labels)\n",
        "\n",
        "      # Backward and optimize\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if (i+1) % 200 == 0:\n",
        "        print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "  print('Finished Training---------------------')"
      ],
      "metadata": {
        "id": "k8ku7nlNhwlc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SaveModel(model):\n",
        "  PATH = '/content/drive/MyDrive/cnn.pth'\n",
        "  torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "UGa9Z_y5wlhr"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def TestNetwork(model,num_epochs, batch_size,learning_rate,resized_shape,actual_data_path):\n",
        "  with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    n_class_correct = [0 for i in range(10)]\n",
        "    n_class_samples = [0 for i in range(10)]\n",
        "\n",
        "    #Function for image augmentation.Calling Compose returns a transform function that performs image augmentation.\n",
        "    test_transforms = A.Compose([# A.SmallestMaxSize(max_size=350),\n",
        "          # A.CenterCrop(height=256, width=256),\n",
        "          A.Resize(resized_shape,resized_shape),\n",
        "          A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "          ToTensorV2()])\n",
        "    \n",
        "    #Function to create test dataset and returns the test image paths\n",
        "    test_image_paths=CreateTestDataset(actual_data_path)\n",
        "    #Validation Dataset created\n",
        "    # valid_dataset = iNaturalist_12KDataset(valid_image_paths,test_transforms) #test transforms are applied\n",
        "    #Test Dataset created\n",
        "    test_dataset = iNaturalist_12KDataset(test_image_paths,test_transforms)\n",
        "\n",
        "    #Dataloader loads test dataset\n",
        "    test_loader = DataLoader(\n",
        "    test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    \n",
        "    #Dataloader loads validation dataset\n",
        "    # valid_loader = DataLoader(\n",
        "    #     valid_dataset, batch_size=batch_size, shuffle=True)\n",
        "    \n",
        "    for images, labels in test_loader:\n",
        "      if enable_GPU == 1:\n",
        "        images = images.to(Device)\n",
        "        labels = labels.to(Device)\n",
        "      outputs = model(images)\n",
        "      # max returns (value ,index)\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      n_samples += labels.size(0)\n",
        "      n_correct += (predicted == labels).sum().item()\n",
        "      for i in range(predicted.size()[0]):\n",
        "        label = labels[i]\n",
        "        pred = predicted[i]\n",
        "        if (label == pred):\n",
        "            n_class_correct[label] += 1\n",
        "        n_class_samples[label] += 1\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network: {acc} %')\n",
        "\n",
        "    for i in range(10):\n",
        "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
        "        print(f'Accuracy of {classes[i]}: {acc} %')"
      ],
      "metadata": {
        "id": "2NfP-wztztIv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Calculates the input feature for the dense linear layer\n",
        "def LinearInFeatureCalculate(initial_dim,conv_attributes,pool_attributes):\n",
        "  for i in range(5):\n",
        "    D = (initial_dim + 2*conv_attributes[i][\"padding\"] - conv_attributes[i][\"dilation\"]*(conv_attributes[i][\"kernel_size\"]-1) - 1)//(conv_attributes[i][\"stride\"]) + 1\n",
        "    D = D//pool_attributes[i][\"stride\"]\n",
        "    initial_dim = D\n",
        "  return D\n"
      ],
      "metadata": {
        "id": "lBScrogGDsZv"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main function**"
      ],
      "metadata": {
        "id": "KS9k_0LHD9Y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  print(\"Hello\")\n",
        "  resized_shape = 256\n",
        "\n",
        "  ##Hyper-parameters of the model training like number of epochs, batch size, learning rate\n",
        "  num_epochs=1\n",
        "  batch_size=64\n",
        "  learning_rate=0.001\n",
        "  optimizer_name = \"Adam\"\n",
        "  actual_data_path = \"/content/drive/MyDrive/inaturalist_12K\"\n",
        "\n",
        "  conv_attributes = [{\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1},\n",
        "                     {\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1},\n",
        "                     {\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1},\n",
        "                     {\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1},\n",
        "                     {\"in_channels\":0,\"out_channels\":0,\"kernel_size\":0, \"stride\":1, \"padding\":0, \"dilation\":1}]\n",
        "  \n",
        "  \n",
        "  ##Attributes for 1st Convolution Layer\n",
        "  conv_attributes[0][\"in_channels\"]=3\n",
        "  conv_attributes[0][\"out_channels\"]=6\n",
        "  conv_attributes[0][\"kernel_size\"]=3\n",
        "\n",
        "  ##Attributes for 2nd Convolution Layer\n",
        "  conv_attributes[1][\"in_channels\"]=6\n",
        "  conv_attributes[1][\"out_channels\"]=12\n",
        "  conv_attributes[1][\"kernel_size\"]=3\n",
        "\n",
        "  ##Attributes for 3rd Convolution Layer\n",
        "  conv_attributes[2][\"in_channels\"]=12\n",
        "  conv_attributes[2][\"out_channels\"]=16\n",
        "  conv_attributes[2][\"kernel_size\"]=5\n",
        "\n",
        "  ##Attributes for 4th Convolution Layer\n",
        "  conv_attributes[3][\"in_channels\"]=16\n",
        "  conv_attributes[3][\"out_channels\"]=32\n",
        "  conv_attributes[3][\"kernel_size\"]=5\n",
        "\n",
        "  ##Attributes for 5th Convolution Layer\n",
        "  conv_attributes[4][\"in_channels\"]=32\n",
        "  conv_attributes[4][\"out_channels\"]=32\n",
        "  conv_attributes[4][\"kernel_size\"]=7\n",
        "\n",
        "  pool_attributes = [{\"kernel_size\":1, \"stride\": 1},\n",
        "                     {\"kernel_size\":1, \"stride\": 1},\n",
        "                     {\"kernel_size\":1, \"stride\": 1},\n",
        "                     {\"kernel_size\":1, \"stride\": 1},\n",
        "                     {\"kernel_size\":1, \"stride\": 1}]\n",
        "\n",
        "  ##Attributes for 1st Pooling Layer\n",
        "  pool_attributes[0][\"kernel_size\"]=2\n",
        "  pool_attributes[0][\"stride\"]=2\n",
        "\n",
        "  ##Attributes for 2nd Pooling Layer\n",
        "  pool_attributes[1][\"kernel_size\"]=2\n",
        "  pool_attributes[1][\"stride\"]=2\n",
        "  \n",
        "  ##Attributes for 3rd Pooling Layer\n",
        "  pool_attributes[2][\"kernel_size\"]=2\n",
        "  pool_attributes[2][\"stride\"]=2\n",
        "\n",
        "  ##Attributes for 4th Pooling Layer\n",
        "  pool_attributes[3][\"kernel_size\"]=2\n",
        "  pool_attributes[3][\"stride\"]=2\n",
        "\n",
        "  ##Attributes for 5th Pooling Layer\n",
        "  pool_attributes[4][\"kernel_size\"]=2\n",
        "  pool_attributes[4][\"stride\"]=2\n",
        "\n",
        " ##Calculating the input dimension for the Dense Linear layer\n",
        "  final_dim=LinearInFeatureCalculate(256,conv_attributes,pool_attributes) #height,width of the dense layer\n",
        "  in_feature = (final_dim ** 2) * conv_attributes[4][\"out_channels\"] #number of input nodes in the dense layer\n",
        "  print(in_feature)\n",
        "\n",
        "  #If the enable_GPU flag is on then the run will use GPU\n",
        "  if enable_GPU == 1:\n",
        "    model = CnnModel(conv_attributes, pool_attributes,in_feature).to(Device)\n",
        "  else :\n",
        "    model = CnnModel(conv_attributes, pool_attributes,in_feature)\n",
        "  print(model)\n",
        "\n",
        "  #Function for training the model with parameters model,num_epochs, batch_size,learning_rate,optimizer_name\n",
        "  TrainNetwork(model,num_epochs, batch_size,learning_rate,optimizer_name,resized_shape,actual_data_path)\n",
        "  #Function for testing the model accuracy on the test data with parameters model,num_epochs, batch_size,learning_rate\n",
        "  TestNetwork(model,num_epochs, batch_size,learning_rate,resized_shape,actual_data_path)"
      ],
      "metadata": {
        "id": "pejeGIEMD87w"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if  __name__ ==\"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxEPk_H5FT4_",
        "outputId": "7977a4c2-a28b-41df-bc2b-2aee105a629d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "288\n",
            "CnnModel(\n",
            "  (conv1): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 12, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv2d(12, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv4): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv5): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1))\n",
            "  (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=288, out_features=10, bias=True)\n",
            ")\n",
            "Finished Training---------------------\n",
            "Accuracy of the network: 20.5 %\n",
            "Accuracy of Arachnida: 3.5 %\n",
            "Accuracy of Amphibia: 18.5 %\n",
            "Accuracy of Fungi: 10.5 %\n",
            "Accuracy of Animalia: 4.0 %\n",
            "Accuracy of Mollusca: 0.0 %\n",
            "Accuracy of Mammalia: 1.0 %\n",
            "Accuracy of Plantae: 42.5 %\n",
            "Accuracy of Aves: 60.0 %\n",
            "Accuracy of Insecta: 16.5 %\n",
            "Accuracy of Reptilia: 48.5 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "#######################################################\n",
        "#                  Visualize Dataset\n",
        "#         Images are plotted after augmentation\n",
        "#######################################################\n",
        "\n",
        "def visualize_augmentations(dataset, idx=0, samples=10, cols=5, random_img = False):\n",
        "    \n",
        "    dataset = copy.deepcopy(dataset)\n",
        "    #we remove the normalize and tensor conversion from our augmentation pipeline\n",
        "    dataset.transform = A.Compose([t for t in dataset.transform if not isinstance(t, (A.Normalize, ToTensorV2))])\n",
        "    rows = samples // cols\n",
        "    \n",
        "        \n",
        "    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(12, 8))\n",
        "    for i in range(samples):\n",
        "        if random_img:\n",
        "            idx = np.random.randint(1,len(train_image_paths))\n",
        "        image, lab = dataset[idx]\n",
        "        ax.ravel()[i].imshow(image)\n",
        "        ax.ravel()[i].set_axis_off()\n",
        "        ax.ravel()[i].set_title(idx_to_class[lab])\n",
        "    plt.tight_layout(pad=1)\n",
        "    plt.show()    \n",
        "\n",
        "visualize_augmentations(train_dataset,np.random.randint(1,len(train_image_paths)), random_img = True)\n"
      ],
      "metadata": {
        "id": "VEb6TwA9WJnz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}